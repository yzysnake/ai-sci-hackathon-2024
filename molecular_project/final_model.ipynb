{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-08T06:53:06.559616Z",
     "start_time": "2024-05-08T06:53:06.555858Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, BatchNorm, JumpingKnowledge\n",
    "import torch.nn.functional as F\n",
    "import helper\n",
    "import data_preprocessing_training"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a Dataset class\n",
    "class MolecularGraphTrain(Dataset):\n",
    "    def __init__(self, cleaned_data, transform=None, pre_transform=None):\n",
    "        super(MolecularGraphTrain, self).__init__(transform=transform, pre_transform=pre_transform)\n",
    "        self.graphs = list(cleaned_data.values())\n",
    "        self._indices = range(len(self.graphs))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._indices)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        graph_info = self.graphs[idx]\n",
    "        return self.create_pyg_data(graph_info)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.get(self._indices[idx])\n",
    "        data = data if self.transform is None else self.transform(data)\n",
    "        return data\n",
    "    \n",
    "    def create_pyg_data(self, graph_info):\n",
    "        # Extract nodes and edges from the graph information\n",
    "        node_id_feature = graph_info[\"node_id_feature\"]\n",
    "        edge_features = graph_info[\"edge_features\"]\n",
    "        target_variable = graph_info[\"target_variable\"]\n",
    "    \n",
    "        # Create the node feature matrix\n",
    "        node_ids = sorted(node_id_feature.keys())\n",
    "        node_features = []\n",
    "        for node_id in node_ids:\n",
    "            features = [\n",
    "                node_id_feature[node_id][\"atomic\"],\n",
    "                node_id_feature[node_id][\"valence\"],\n",
    "                node_id_feature[node_id][\"formal_charge\"],\n",
    "                node_id_feature[node_id][\"aromatic\"],\n",
    "                node_id_feature[node_id][\"hybridization\"],\n",
    "                node_id_feature[node_id][\"radical_electrons\"]\n",
    "            ]\n",
    "            node_features.append(features)\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "        # Create the edge list\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        for edge in edge_features:\n",
    "            edge_index.append([edge[\"source\"], edge[\"target\"]])\n",
    "            edge_attr.append([\n",
    "                edge[\"type\"],\n",
    "                edge[\"stereo\"],\n",
    "                edge[\"aromatic\"],\n",
    "                edge[\"conjugated\"]\n",
    "            ])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "        # Create the target variable tensor\n",
    "        target_list = [target_variable[node_id] for node_id in node_ids]\n",
    "        y = torch.tensor([[t[\"mass\"], t[\"charge\"], t[\"sigma\"], t[\"epsilon\"]] for t in target_list], dtype=torch.float)\n",
    "    \n",
    "        # Return the graph as a Data object\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T06:53:07.360975Z",
     "start_time": "2024-05-08T06:53:07.355638Z"
    }
   },
   "id": "99bba2df99121011",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a Dataset class\n",
    "class MolecularGraphTest(Dataset):\n",
    "    def __init__(self, cleaned_data, transform=None, pre_transform=None):\n",
    "        super(MolecularGraphTest, self).__init__(transform=transform, pre_transform=pre_transform)\n",
    "        self.graphs = list(cleaned_data.values())\n",
    "        self._indices = range(len(self.graphs))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._indices)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        graph_info = self.graphs[idx]\n",
    "        return self.create_pyg_data(graph_info)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.get(self._indices[idx])\n",
    "        data = data if self.transform is None else self.transform(data)\n",
    "        return data\n",
    "    \n",
    "    def create_pyg_data(self, graph_info):\n",
    "        # Extract nodes and edges from the graph information\n",
    "        node_id_feature = graph_info[\"node_id_feature\"]\n",
    "        edge_features = graph_info[\"edge_features\"]\n",
    "    \n",
    "        # Create the node feature matrix\n",
    "        node_ids = sorted(node_id_feature.keys())\n",
    "        node_features = []\n",
    "        for node_id in node_ids:\n",
    "            features = [\n",
    "                node_id_feature[node_id][\"atomic\"],\n",
    "                node_id_feature[node_id][\"valence\"],\n",
    "                node_id_feature[node_id][\"formal_charge\"],\n",
    "                node_id_feature[node_id][\"aromatic\"],\n",
    "                node_id_feature[node_id][\"hybridization\"],\n",
    "                node_id_feature[node_id][\"radical_electrons\"]\n",
    "            ]\n",
    "            node_features.append(features)\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "        # Create the edge list\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        for edge in edge_features:\n",
    "            edge_index.append([edge[\"source\"], edge[\"target\"]])\n",
    "            edge_attr.append([\n",
    "                edge[\"type\"],\n",
    "                edge[\"stereo\"],\n",
    "                edge[\"aromatic\"],\n",
    "                edge[\"conjugated\"]\n",
    "            ])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "        # Return the graph as a Data object\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T06:56:45.409635Z",
     "start_time": "2024-05-08T06:56:45.403393Z"
    }
   },
   "id": "dc90f28c44b2703",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NodeEmbedding(nn.Module):\n",
    "    def __init__(self, num_atomic, num_valence, num_formal_charge, num_hybridization, num_radical_electrons, embedding_dim):\n",
    "        super(NodeEmbedding, self).__init__()\n",
    "        self.atomic_embedding = nn.Embedding(num_atomic, embedding_dim)\n",
    "        self.valence_embedding = nn.Embedding(num_valence, embedding_dim)\n",
    "        self.formal_charge_embedding = nn.Embedding(num_formal_charge, embedding_dim)\n",
    "        self.hybridization_embedding = nn.Embedding(num_hybridization, embedding_dim)\n",
    "        self.radical_electrons_embedding = nn.Embedding(num_radical_electrons, embedding_dim)\n",
    "\n",
    "    def forward(self, atomic, valence, formal_charge, aromatic, hybridization, radical_electrons):\n",
    "        atomic_embed = self.atomic_embedding(atomic)\n",
    "        valence_embed = self.valence_embedding(valence)\n",
    "        formal_charge_embed = self.formal_charge_embedding(formal_charge)\n",
    "        hybridization_embed = self.hybridization_embedding(hybridization)\n",
    "        radical_electrons_embed = self.radical_electrons_embedding(radical_electrons)\n",
    "\n",
    "        # Concatenate boolean features\n",
    "        other_features = torch.stack([aromatic], dim=1).float()\n",
    "\n",
    "        # Concatenate all features together\n",
    "        return torch.cat([atomic_embed, valence_embed, formal_charge_embed, hybridization_embed, radical_electrons_embed, other_features], dim=1)\n",
    "\n",
    "class EdgeEmbedding(nn.Module):\n",
    "    def __init__(self, num_type, num_stereo, embedding_dim):\n",
    "        super(EdgeEmbedding, self).__init__()\n",
    "        self.type_embedding = nn.Embedding(num_type, embedding_dim)\n",
    "        self.stereo_embedding = nn.Embedding(num_stereo, embedding_dim)\n",
    "\n",
    "    def forward(self, type_, stereo, aromatic, conjugated):\n",
    "        type_embed = self.type_embedding(type_)\n",
    "        stereo_embed = self.stereo_embedding(stereo)\n",
    "\n",
    "        # Concatenate boolean features directly\n",
    "        other_features = torch.stack([aromatic, conjugated], dim=1).float()\n",
    "\n",
    "        # Concatenate all features together\n",
    "        return torch.cat([type_embed, stereo_embed, other_features], dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T06:56:46.914246Z",
     "start_time": "2024-05-08T06:56:46.909354Z"
    }
   },
   "id": "be5c1beb5ca377f3",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ImprovedGNNWithEmbeddings(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, node_embedding_dim, edge_embedding_dim, hidden_dim, output_dim, num_layers = 6, num_atomic = 12, num_valence = 7, num_hybridization = 5, num_type = 4, num_stereo = 3 ,num_formal_charge = 3, num_radical_electrons = 1):\n",
    "        super(ImprovedGNNWithEmbeddings, self).__init__()\n",
    "        self.node_embedding = NodeEmbedding(num_atomic, num_valence, num_formal_charge, num_hybridization, num_radical_electrons, node_embedding_dim)\n",
    "        self.edge_embedding = EdgeEmbedding(num_type, num_stereo, edge_embedding_dim)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Define the first GINEConv layer, with the correct edge_dim specified\n",
    "        self.convs.append(GINEConv(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(node_input_dim, hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "            ),\n",
    "            edge_dim=edge_input_dim, train_eps = True\n",
    "        ))\n",
    "        self.norms.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        # Additional GINEConv layers, each with the correct edge_dim\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GINEConv(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "                ),\n",
    "                edge_dim=edge_input_dim, train_eps = True\n",
    "            ))\n",
    "            self.norms.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        # Jumping Knowledge mechanism\n",
    "        self.jump = JumpingKnowledge(mode=\"cat\")\n",
    "\n",
    "        # Final fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim * num_layers, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        features = []\n",
    "\n",
    "        # Pass through GINEConv layers and apply batch normalization\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = F.relu(norm(x))\n",
    "            features.append(x)\n",
    "\n",
    "        # Apply Jumping Knowledge (JK) to concatenate all layers\n",
    "        x = self.jump(features)\n",
    "\n",
    "        # Directly pass through the linear layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T06:56:47.518162Z",
     "start_time": "2024-05-08T06:56:47.513064Z"
    }
   },
   "id": "e914ac2e07356efc",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import data_preprocessing_testing\n",
    "\n",
    "train_data = helper.load_data_from_file(\"data.json\")\n",
    "cleaned_data_train = data_preprocessing_training.extract_clean_data(train_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T06:56:49.706499Z",
     "start_time": "2024-05-08T06:56:48.106681Z"
    }
   },
   "id": "a21590047cf6cde4",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ImprovedGNNWithEmbeddings(\n  (node_embedding): NodeEmbedding(\n    (atomic_embedding): Embedding(12, 32)\n    (valence_embedding): Embedding(7, 32)\n    (formal_charge_embedding): Embedding(3, 32)\n    (hybridization_embedding): Embedding(5, 32)\n    (radical_electrons_embedding): Embedding(1, 32)\n  )\n  (edge_embedding): EdgeEmbedding(\n    (type_embedding): Embedding(4, 32)\n    (stereo_embedding): Embedding(3, 32)\n  )\n  (convs): ModuleList(\n    (0): GINEConv(nn=Sequential(\n      (0): Linear(in_features=6, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n    ))\n    (1-5): 5 x GINEConv(nn=Sequential(\n      (0): Linear(in_features=256, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n    ))\n  )\n  (norms): ModuleList(\n    (0-5): 6 x BatchNorm(256)\n  )\n  (jump): JumpingKnowledge(cat)\n  (fc1): Linear(in_features=1536, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=4, bias=True)\n)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "node_input_dim = 6  # node feature dimension\n",
    "edge_input_dim = 4  # edge feature dimension\n",
    "hidden_dim = 256\n",
    "output_dim = 4  # The number of outputs (mass, charge, sigma, epsilon)\n",
    "num_epochs = 500\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = ImprovedGNNWithEmbeddings(node_embedding_dim = 32, edge_embedding_dim = 32, hidden_dim = hidden_dim, output_dim = 4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T07:03:29.806007Z",
     "start_time": "2024-05-08T07:03:29.787276Z"
    }
   },
   "id": "f214a9335bdd46c3",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 17.6491\n",
      "Epoch [2/300], Loss: 5.0653\n",
      "Epoch [3/300], Loss: 1.0530\n",
      "Epoch [4/300], Loss: 0.4029\n",
      "Epoch [5/300], Loss: 0.2339\n",
      "Epoch [6/300], Loss: 0.1642\n",
      "Epoch [7/300], Loss: 0.1334\n",
      "Epoch [8/300], Loss: 0.1229\n",
      "Epoch [9/300], Loss: 0.0945\n",
      "Epoch [10/300], Loss: 0.0752\n",
      "Epoch [11/300], Loss: 0.0699\n",
      "Epoch [12/300], Loss: 0.0709\n",
      "Epoch [13/300], Loss: 0.0566\n",
      "Epoch [14/300], Loss: 0.0525\n",
      "Epoch [15/300], Loss: 0.0570\n",
      "Epoch [16/300], Loss: 0.0384\n",
      "Epoch [17/300], Loss: 0.0412\n",
      "Epoch [18/300], Loss: 0.0412\n",
      "Epoch [19/300], Loss: 0.0358\n",
      "Epoch [20/300], Loss: 0.0365\n",
      "Epoch [21/300], Loss: 0.0289\n",
      "Epoch [22/300], Loss: 0.0307\n",
      "Epoch [23/300], Loss: 0.0308\n",
      "Epoch [24/300], Loss: 0.0272\n",
      "Epoch [25/300], Loss: 0.0297\n",
      "Epoch [26/300], Loss: 0.0291\n",
      "Epoch [27/300], Loss: 0.0327\n",
      "Epoch [28/300], Loss: 0.0300\n",
      "Epoch [29/300], Loss: 0.0258\n",
      "Epoch [30/300], Loss: 0.0221\n",
      "Epoch [31/300], Loss: 0.0208\n",
      "Epoch [32/300], Loss: 0.0256\n",
      "Epoch [33/300], Loss: 0.0227\n",
      "Epoch [34/300], Loss: 0.0183\n",
      "Epoch [35/300], Loss: 0.0163\n",
      "Epoch [36/300], Loss: 0.0225\n",
      "Epoch [37/300], Loss: 0.0212\n",
      "Epoch [38/300], Loss: 0.0204\n",
      "Epoch [39/300], Loss: 0.0222\n",
      "Epoch [40/300], Loss: 0.0238\n",
      "Epoch [41/300], Loss: 0.0261\n",
      "Epoch [42/300], Loss: 0.0225\n",
      "Epoch [43/300], Loss: 0.0218\n",
      "Epoch [44/300], Loss: 0.0165\n",
      "Epoch [45/300], Loss: 0.0258\n",
      "Epoch [46/300], Loss: 0.0233\n",
      "Epoch [47/300], Loss: 0.0181\n",
      "Epoch [48/300], Loss: 0.0204\n",
      "Epoch [49/300], Loss: 0.0151\n",
      "Epoch [50/300], Loss: 0.0139\n",
      "Epoch [51/300], Loss: 0.0127\n",
      "Epoch [52/300], Loss: 0.0198\n",
      "Epoch [53/300], Loss: 0.0214\n",
      "Epoch [54/300], Loss: 0.0142\n",
      "Epoch [55/300], Loss: 0.0104\n",
      "Epoch [56/300], Loss: 0.0116\n",
      "Epoch [57/300], Loss: 0.0108\n",
      "Epoch [58/300], Loss: 0.0126\n",
      "Epoch [59/300], Loss: 0.0127\n",
      "Epoch [60/300], Loss: 0.0132\n",
      "Epoch [61/300], Loss: 0.0171\n",
      "Epoch [62/300], Loss: 0.0164\n",
      "Epoch [63/300], Loss: 0.0127\n",
      "Epoch [64/300], Loss: 0.0113\n",
      "Epoch [65/300], Loss: 0.0135\n",
      "Epoch [66/300], Loss: 0.0094\n",
      "Epoch [67/300], Loss: 0.0098\n",
      "Epoch [68/300], Loss: 0.0101\n",
      "Epoch [69/300], Loss: 0.0175\n",
      "Epoch [70/300], Loss: 0.0089\n",
      "Epoch [71/300], Loss: 0.0097\n",
      "Epoch [72/300], Loss: 0.0088\n",
      "Epoch [73/300], Loss: 0.0112\n",
      "Epoch [74/300], Loss: 0.0103\n",
      "Epoch [75/300], Loss: 0.0124\n",
      "Epoch [76/300], Loss: 0.0089\n",
      "Epoch [77/300], Loss: 0.0095\n",
      "Epoch [78/300], Loss: 0.0100\n",
      "Epoch [79/300], Loss: 0.0095\n",
      "Epoch [80/300], Loss: 0.0097\n",
      "Epoch [81/300], Loss: 0.0159\n",
      "Epoch [82/300], Loss: 0.0229\n",
      "Epoch [83/300], Loss: 0.0191\n",
      "Epoch [84/300], Loss: 0.0201\n",
      "Epoch [85/300], Loss: 0.0139\n",
      "Epoch [86/300], Loss: 0.0164\n",
      "Epoch [87/300], Loss: 0.0147\n",
      "Epoch [88/300], Loss: 0.0108\n",
      "Epoch [89/300], Loss: 0.0081\n",
      "Epoch [90/300], Loss: 0.0082\n",
      "Epoch [91/300], Loss: 0.0096\n",
      "Epoch [92/300], Loss: 0.0099\n",
      "Epoch [93/300], Loss: 0.0079\n",
      "Epoch [94/300], Loss: 0.0094\n",
      "Epoch [95/300], Loss: 0.0100\n",
      "Epoch [96/300], Loss: 0.0081\n",
      "Epoch [97/300], Loss: 0.0109\n",
      "Epoch [98/300], Loss: 0.0084\n",
      "Epoch [99/300], Loss: 0.0105\n",
      "Epoch [100/300], Loss: 0.0127\n",
      "Epoch [101/300], Loss: 0.0098\n",
      "Epoch [102/300], Loss: 0.0084\n",
      "Epoch [103/300], Loss: 0.0109\n",
      "Epoch [104/300], Loss: 0.0100\n",
      "Epoch [105/300], Loss: 0.0105\n",
      "Epoch [106/300], Loss: 0.0107\n",
      "Epoch [107/300], Loss: 0.0112\n",
      "Epoch [108/300], Loss: 0.0136\n",
      "Epoch [109/300], Loss: 0.0075\n",
      "Epoch [110/300], Loss: 0.0075\n",
      "Epoch [111/300], Loss: 0.0093\n",
      "Epoch [112/300], Loss: 0.0093\n",
      "Epoch [113/300], Loss: 0.0124\n",
      "Epoch [114/300], Loss: 0.0177\n",
      "Epoch [115/300], Loss: 0.0126\n",
      "Epoch [116/300], Loss: 0.0120\n",
      "Epoch [117/300], Loss: 0.0127\n",
      "Epoch [118/300], Loss: 0.0091\n",
      "Epoch [119/300], Loss: 0.0061\n",
      "Epoch [120/300], Loss: 0.0089\n",
      "Epoch [121/300], Loss: 0.0107\n",
      "Epoch [122/300], Loss: 0.0075\n",
      "Epoch [123/300], Loss: 0.0072\n",
      "Epoch [124/300], Loss: 0.0084\n",
      "Epoch [125/300], Loss: 0.0071\n",
      "Epoch [126/300], Loss: 0.0068\n",
      "Epoch [127/300], Loss: 0.0052\n",
      "Epoch [128/300], Loss: 0.0109\n",
      "Epoch [129/300], Loss: 0.0072\n",
      "Epoch [130/300], Loss: 0.0080\n",
      "Epoch [131/300], Loss: 0.0079\n",
      "Epoch [132/300], Loss: 0.0066\n",
      "Epoch [133/300], Loss: 0.0068\n",
      "Epoch [134/300], Loss: 0.0088\n",
      "Epoch [135/300], Loss: 0.0085\n",
      "Epoch [136/300], Loss: 0.0089\n",
      "Epoch [137/300], Loss: 0.0061\n",
      "Epoch [138/300], Loss: 0.0050\n",
      "Epoch [139/300], Loss: 0.0061\n",
      "Epoch [140/300], Loss: 0.0081\n",
      "Epoch [141/300], Loss: 0.0063\n",
      "Epoch [142/300], Loss: 0.0065\n",
      "Epoch [143/300], Loss: 0.0074\n",
      "Epoch [144/300], Loss: 0.0061\n",
      "Epoch [145/300], Loss: 0.0063\n",
      "Epoch [146/300], Loss: 0.0061\n",
      "Epoch [147/300], Loss: 0.0074\n",
      "Epoch [148/300], Loss: 0.0072\n",
      "Epoch [149/300], Loss: 0.0068\n",
      "Epoch [150/300], Loss: 0.0058\n",
      "Epoch [151/300], Loss: 0.0058\n",
      "Epoch [152/300], Loss: 0.0102\n",
      "Epoch [153/300], Loss: 0.0080\n",
      "Epoch [154/300], Loss: 0.0054\n",
      "Epoch [155/300], Loss: 0.0051\n",
      "Epoch [156/300], Loss: 0.0048\n",
      "Epoch [157/300], Loss: 0.0084\n",
      "Epoch [158/300], Loss: 0.0071\n",
      "Epoch [159/300], Loss: 0.0056\n",
      "Epoch [160/300], Loss: 0.0047\n",
      "Epoch [161/300], Loss: 0.0050\n",
      "Epoch [162/300], Loss: 0.0053\n",
      "Epoch [163/300], Loss: 0.0085\n",
      "Epoch [164/300], Loss: 0.0066\n",
      "Epoch [165/300], Loss: 0.0069\n",
      "Epoch [166/300], Loss: 0.0079\n",
      "Epoch [167/300], Loss: 0.0060\n",
      "Epoch [168/300], Loss: 0.0046\n",
      "Epoch [169/300], Loss: 0.0051\n",
      "Epoch [170/300], Loss: 0.0089\n",
      "Epoch [171/300], Loss: 0.0118\n",
      "Epoch [172/300], Loss: 0.0055\n",
      "Epoch [173/300], Loss: 0.0046\n",
      "Epoch [174/300], Loss: 0.0060\n",
      "Epoch [175/300], Loss: 0.0052\n",
      "Epoch [176/300], Loss: 0.0079\n",
      "Epoch [177/300], Loss: 0.0101\n",
      "Epoch [178/300], Loss: 0.0127\n",
      "Epoch [179/300], Loss: 0.0112\n",
      "Epoch [180/300], Loss: 0.0076\n",
      "Epoch [181/300], Loss: 0.0125\n",
      "Epoch [182/300], Loss: 0.0060\n",
      "Epoch [183/300], Loss: 0.0069\n",
      "Epoch [184/300], Loss: 0.0087\n",
      "Epoch [185/300], Loss: 0.0055\n",
      "Epoch [186/300], Loss: 0.0151\n",
      "Epoch [187/300], Loss: 0.0079\n",
      "Epoch [188/300], Loss: 0.0085\n",
      "Epoch [189/300], Loss: 0.0077\n",
      "Epoch [190/300], Loss: 0.0072\n",
      "Epoch [191/300], Loss: 0.0047\n",
      "Epoch [192/300], Loss: 0.0053\n",
      "Epoch [193/300], Loss: 0.0070\n",
      "Epoch [194/300], Loss: 0.0059\n",
      "Epoch [195/300], Loss: 0.0052\n",
      "Epoch [196/300], Loss: 0.0061\n",
      "Epoch [197/300], Loss: 0.0075\n",
      "Epoch [198/300], Loss: 0.0071\n",
      "Epoch [199/300], Loss: 0.0078\n",
      "Epoch [200/300], Loss: 0.0068\n",
      "Epoch [201/300], Loss: 0.0052\n",
      "Epoch [202/300], Loss: 0.0050\n",
      "Epoch [203/300], Loss: 0.0071\n",
      "Epoch [204/300], Loss: 0.0071\n",
      "Epoch [205/300], Loss: 0.0062\n",
      "Epoch [206/300], Loss: 0.0053\n",
      "Epoch [207/300], Loss: 0.0069\n",
      "Epoch [208/300], Loss: 0.0064\n",
      "Epoch [209/300], Loss: 0.0056\n",
      "Epoch [210/300], Loss: 0.0069\n",
      "Epoch [211/300], Loss: 0.0053\n",
      "Epoch [212/300], Loss: 0.0106\n",
      "Epoch [213/300], Loss: 0.0107\n",
      "Epoch [214/300], Loss: 0.0085\n",
      "Epoch [215/300], Loss: 0.0056\n",
      "Epoch [216/300], Loss: 0.0058\n",
      "Epoch [217/300], Loss: 0.0109\n",
      "Epoch [218/300], Loss: 0.0095\n",
      "Epoch [219/300], Loss: 0.0086\n",
      "Epoch [220/300], Loss: 0.0059\n",
      "Epoch [221/300], Loss: 0.0069\n",
      "Epoch [222/300], Loss: 0.0061\n",
      "Epoch [223/300], Loss: 0.0052\n",
      "Epoch [224/300], Loss: 0.0050\n",
      "Epoch [225/300], Loss: 0.0083\n",
      "Epoch [226/300], Loss: 0.0120\n",
      "Epoch [227/300], Loss: 0.0071\n",
      "Epoch [228/300], Loss: 0.0079\n",
      "Epoch [229/300], Loss: 0.0120\n",
      "Epoch [230/300], Loss: 0.0105\n",
      "Epoch [231/300], Loss: 0.0083\n",
      "Epoch [232/300], Loss: 0.0050\n",
      "Epoch [233/300], Loss: 0.0059\n",
      "Epoch [234/300], Loss: 0.0084\n",
      "Epoch [235/300], Loss: 0.0135\n",
      "Epoch [236/300], Loss: 0.0101\n",
      "Epoch [237/300], Loss: 0.0081\n",
      "Epoch [238/300], Loss: 0.0067\n",
      "Epoch [239/300], Loss: 0.0075\n",
      "Epoch [240/300], Loss: 0.0071\n",
      "Epoch [241/300], Loss: 0.0087\n",
      "Epoch [242/300], Loss: 0.0082\n",
      "Epoch [243/300], Loss: 0.0095\n",
      "Epoch [244/300], Loss: 0.0051\n",
      "Epoch [245/300], Loss: 0.0074\n",
      "Epoch [246/300], Loss: 0.0048\n",
      "Epoch [247/300], Loss: 0.0055\n",
      "Epoch [248/300], Loss: 0.0052\n",
      "Epoch [249/300], Loss: 0.0048\n",
      "Epoch [250/300], Loss: 0.0058\n",
      "Epoch [251/300], Loss: 0.0053\n",
      "Epoch [252/300], Loss: 0.0041\n",
      "Epoch [253/300], Loss: 0.0041\n",
      "Epoch [254/300], Loss: 0.0050\n",
      "Epoch [255/300], Loss: 0.0057\n",
      "Epoch [256/300], Loss: 0.0042\n",
      "Epoch [257/300], Loss: 0.0041\n",
      "Epoch [258/300], Loss: 0.0037\n",
      "Epoch [259/300], Loss: 0.0038\n",
      "Epoch [260/300], Loss: 0.0068\n",
      "Epoch [261/300], Loss: 0.0065\n",
      "Epoch [262/300], Loss: 0.0065\n",
      "Epoch [263/300], Loss: 0.0056\n",
      "Epoch [264/300], Loss: 0.0046\n",
      "Epoch [265/300], Loss: 0.0055\n",
      "Epoch [266/300], Loss: 0.0092\n",
      "Epoch [267/300], Loss: 0.0070\n",
      "Epoch [268/300], Loss: 0.0051\n",
      "Epoch [269/300], Loss: 0.0046\n",
      "Epoch [270/300], Loss: 0.0052\n",
      "Epoch [271/300], Loss: 0.0046\n",
      "Epoch [272/300], Loss: 0.0067\n",
      "Epoch [273/300], Loss: 0.0048\n",
      "Epoch [274/300], Loss: 0.0043\n",
      "Epoch [275/300], Loss: 0.0045\n",
      "Epoch [276/300], Loss: 0.0035\n",
      "Epoch [277/300], Loss: 0.0045\n",
      "Epoch [278/300], Loss: 0.0039\n",
      "Epoch [279/300], Loss: 0.0045\n",
      "Epoch [280/300], Loss: 0.0043\n",
      "Epoch [281/300], Loss: 0.0040\n",
      "Epoch [282/300], Loss: 0.0043\n",
      "Epoch [283/300], Loss: 0.0047\n",
      "Epoch [284/300], Loss: 0.0044\n",
      "Epoch [285/300], Loss: 0.0047\n",
      "Epoch [286/300], Loss: 0.0057\n",
      "Epoch [287/300], Loss: 0.0046\n",
      "Epoch [288/300], Loss: 0.0051\n",
      "Epoch [289/300], Loss: 0.0073\n",
      "Epoch [290/300], Loss: 0.0102\n",
      "Epoch [291/300], Loss: 0.0051\n",
      "Epoch [292/300], Loss: 0.0043\n",
      "Epoch [293/300], Loss: 0.0045\n",
      "Epoch [294/300], Loss: 0.0044\n",
      "Epoch [295/300], Loss: 0.0043\n",
      "Epoch [296/300], Loss: 0.0056\n",
      "Epoch [297/300], Loss: 0.0047\n",
      "Epoch [298/300], Loss: 0.0048\n",
      "Epoch [299/300], Loss: 0.0065\n",
      "Epoch [300/300], Loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "# Set up the training and testing DataLoaders using PyTorch Geometric DataLoader\n",
    "train_dataset = MolecularGraphTrain(cleaned_data_train)\n",
    "\n",
    "\n",
    "# Use a smaller batch size for better memory management, particularly with graph data\n",
    "train_loader = PyGDataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Move data to the device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch.y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Model evaluation on the test dataset\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Convert predictions to a suitable format if needed\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T07:05:49.479945Z",
     "start_time": "2024-05-08T07:03:30.754568Z"
    }
   },
   "id": "32da7bc662878bf7",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"improved_gnn_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T07:06:54.094780Z",
     "start_time": "2024-05-08T07:06:54.029025Z"
    }
   },
   "id": "8db5402e4ebd4802",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_data = helper.load_data_from_file(\"permutation_masked.json\")\n",
    "cleaned_data_test = data_preprocessing_testing.extract_clean_data(test_data)\n",
    "test_dataset = MolecularGraphTest(cleaned_data_test)\n",
    "test_loader = PyGDataLoader(test_dataset, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T08:27:23.697356Z",
     "start_time": "2024-05-08T08:27:23.654920Z"
    }
   },
   "id": "f3efe8347cb6180e",
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move data to the device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass (prediction)\n",
    "        output = model(batch)\n",
    "\n",
    "        # Append the predictions\n",
    "        predictions.append(output.cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T08:27:24.376411Z",
     "start_time": "2024-05-08T08:27:24.212162Z"
    }
   },
   "id": "1abcf5bbc287eb10",
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.58286982e+01, -2.81746477e-01,  2.74159253e-01,\n         5.95377922e-01],\n       [ 1.21445084e+01, -1.17456332e-01,  3.52593303e-01,\n         3.21284503e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 9.79994118e-01,  8.54815692e-02,  2.37182260e-01,\n         1.16635114e-01],\n       [ 1.21018267e+01, -1.53920308e-01,  3.57927710e-01,\n         3.40657890e-01],\n       [ 1.19478636e+01, -7.81530291e-02,  3.47847670e-01,\n         2.97787905e-01],\n       [ 9.18771029e-01,  4.36948031e-01,  4.28940915e-03,\n        -1.28815174e-02],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.20845108e+01, -1.60441294e-01,  3.51223499e-01,\n         2.81991839e-01],\n       [ 9.57518160e-01,  1.08655617e-01,  2.45491982e-01,\n         1.15989536e-01],\n       [ 1.22063856e+01, -6.03287406e-02,  3.32707584e-01,\n         3.14476371e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.20214977e+01, -1.22370139e-01,  3.49919409e-01,\n         2.90170103e-01],\n       [ 1.20724516e+01, -1.56465665e-01,  3.46308947e-01,\n         2.91300416e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.58122692e+01, -2.71976739e-01,  2.82001257e-01,\n         5.93149662e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.58606405e+01, -6.06170654e-01,  2.96688855e-01,\n         6.82624578e-01],\n       [ 1.57747431e+01, -6.06857181e-01,  2.92020738e-01,\n         6.90407157e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.20962687e+01, -1.38700679e-01,  3.57419312e-01,\n         2.89192021e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 1.18276405e+01,  3.74492817e-02,  3.32037985e-01,\n         2.95644730e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 9.98770237e-01,  5.00148349e-02,  2.57928699e-01,\n         1.13780484e-01],\n       [ 1.00640953e+00,  5.41230850e-02,  2.49790296e-01,\n         1.26204252e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.58128548e+01, -2.83079118e-01,  2.75020212e-01,\n         5.91754198e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.98770237e-01,  5.00148349e-02,  2.57928699e-01,\n         1.13780484e-01],\n       [ 1.57971048e+01, -6.07154906e-01,  2.88779676e-01,\n         6.82960510e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.21118002e+01, -1.18900552e-01,  3.45388323e-01,\n         3.00091535e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.50711548e-01,  4.35352474e-01, -7.00891204e-03,\n        -1.01098418e-02],\n       [ 1.19236040e+01, -1.33843020e-01,  3.40793014e-01,\n         3.00964981e-01],\n       [ 1.21394606e+01, -1.72520652e-01,  3.59129131e-01,\n         3.33025128e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.21165981e+01, -1.69401124e-01,  3.50711942e-01,\n         2.79098898e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.19434242e+01,  1.30757801e-02,  3.46881449e-01,\n         2.85829991e-01],\n       [ 1.19434242e+01,  1.30757801e-02,  3.46881449e-01,\n         2.85829991e-01],\n       [ 1.18331995e+01, -1.15335420e-01,  3.39962900e-01,\n         3.12688738e-01],\n       [ 9.78135943e-01,  9.60247368e-02,  2.40665093e-01,\n         1.08123854e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.20457973e+01, -1.41374186e-01,  3.49238276e-01,\n         2.82954931e-01],\n       [ 1.20883913e+01, -1.41201571e-01,  3.50309551e-01,\n         3.07969332e-01],\n       [ 1.18721523e+01, -3.04491110e-02,  3.46625149e-01,\n         2.93670267e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 9.68703926e-01,  1.16164550e-01,  2.36518115e-01,\n         1.22697368e-01],\n       [ 1.57983255e+01, -2.57002085e-01,  2.64150202e-01,\n         5.78144670e-01],\n       [ 1.20174046e+01, -1.52953342e-01,  3.44441593e-01,\n         2.94033021e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 9.65609372e-01,  9.88299102e-02,  2.42661417e-01,\n         1.19186074e-01],\n       [ 1.20667887e+01, -1.13911822e-01,  3.52553338e-01,\n         2.84145892e-01],\n       [ 1.20394306e+01, -1.35618463e-01,  3.49158913e-01,\n         2.80673265e-01],\n       [ 9.86368358e-01,  8.43070596e-02,  2.33588234e-01,\n         1.18101016e-01],\n       [ 1.19434242e+01,  1.30757801e-02,  3.46881449e-01,\n         2.85829991e-01],\n       [ 1.58353615e+01, -3.41179818e-01,  2.53081083e-01,\n         6.16725564e-01],\n       [ 1.21080189e+01, -1.70763507e-01,  3.48250806e-01,\n         2.82465041e-01],\n       [ 1.20695601e+01, -1.85404673e-01,  3.52039218e-01,\n         3.19893748e-01],\n       [ 1.58286982e+01, -2.81746477e-01,  2.74159253e-01,\n         5.95377922e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 1.20616302e+01, -1.12349346e-01,  3.53049994e-01,\n         2.84431785e-01],\n       [ 9.98770237e-01,  5.00148349e-02,  2.57928699e-01,\n         1.13780484e-01],\n       [ 1.18299761e+01, -1.47679254e-01,  3.41119230e-01,\n         3.19592476e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.19519997e+01, -2.52483077e-02,  3.40997577e-01,\n         2.78204381e-01],\n       [ 9.98770237e-01,  5.00148349e-02,  2.57928699e-01,\n         1.13780484e-01],\n       [ 9.98770237e-01,  5.00148349e-02,  2.57928699e-01,\n         1.13780484e-01],\n       [ 9.87144113e-01,  4.28708375e-01, -2.41695531e-03,\n        -1.31824464e-02],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.57822876e+01, -2.58923203e-01,  2.85490543e-01,\n         6.06162071e-01],\n       [ 1.58128548e+01, -2.83079118e-01,  2.75020212e-01,\n         5.91754198e-01],\n       [ 9.70475018e-01,  5.03990315e-02,  2.48270303e-01,\n         1.10584401e-01],\n       [ 1.20394306e+01, -1.35618463e-01,  3.49158913e-01,\n         2.80673265e-01],\n       [ 1.57897358e+01, -2.52258867e-01,  2.63275027e-01,\n         5.78383327e-01]], dtype=float32)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T08:27:24.627322Z",
     "start_time": "2024-05-08T08:27:24.622577Z"
    }
   },
   "id": "db1ac6439ee078b8",
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8afa7adb1688d74e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
