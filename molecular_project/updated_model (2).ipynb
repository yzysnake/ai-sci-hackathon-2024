{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -qq torch_geometric"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fklz1j1vvldf",
    "outputId": "3969de05-7c54-48a8-c9c9-ec421fd1b692"
   },
   "id": "Fklz1j1vvldf",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, BatchNorm, JumpingKnowledge\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, BatchNorm, JumpingKnowledge\n",
    "import torch.nn.functional as F\n",
    "import helper\n",
    "import data_preprocessing_training"
   ],
   "metadata": {
    "id": "6b58946add4d2f07",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:27:25.325078Z",
     "start_time": "2024-05-08T06:27:25.320565Z"
    }
   },
   "id": "6b58946add4d2f07",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = helper.load_data_from_file(\"data.json\")\n",
    "cleaned_data = data_preprocessing_training.extract_clean_data(train_data)"
   ],
   "metadata": {
    "id": "ea80f9bd462fb5d9",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:27:28.783274Z",
     "start_time": "2024-05-08T06:27:26.863424Z"
    }
   },
   "id": "ea80f9bd462fb5d9",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unique_num_atomic': 12, 'unique_num_formal_charge': 3, 'unique_num_valence': 7, 'unique_num_hybridization': 5, 'unique_num_radical_electrons': 1, 'unique_num_type': 4, 'unique_num_stereo': 3}\n"
     ]
    }
   ],
   "source": [
    "def count_unique_attributes(data):\n",
    "    # Sets to store unique values of node attributes\n",
    "    atomic_set = set()\n",
    "    formal_charge_set = set()\n",
    "    valence_set = set()\n",
    "    hybridization_set = set()\n",
    "    radical_electrons_set = set()\n",
    "\n",
    "    # Sets to store unique values of edge attributes\n",
    "    type_set = set()\n",
    "    stereo_set = set()\n",
    "\n",
    "    # Loop through data structure to collect unique values\n",
    "    for smiles, features in data.items():\n",
    "        # Collect node attributes\n",
    "        for node in features['node_id_feature'].values():\n",
    "            atomic_set.add(node['atomic'])\n",
    "            formal_charge_set.add(node['formal_charge'])\n",
    "            valence_set.add(node['valence'])\n",
    "            hybridization_set.add(node['hybridization'])\n",
    "            radical_electrons_set.add(node['radical_electrons'])\n",
    "\n",
    "        # Collect edge attributes\n",
    "        for edge in features['edge_features']:\n",
    "            type_set.add(edge['type'])\n",
    "            stereo_set.add(edge['stereo'])\n",
    "\n",
    "    # Create a result dictionary for easy display\n",
    "    result = {\n",
    "        \"unique_num_atomic\": len(atomic_set),\n",
    "        \"unique_num_formal_charge\": len(formal_charge_set),\n",
    "        \"unique_num_valence\": len(valence_set),\n",
    "        \"unique_num_hybridization\": len(hybridization_set),\n",
    "        \"unique_num_radical_electrons\": len(radical_electrons_set),\n",
    "        \"unique_num_type\": len(type_set),\n",
    "        \"unique_num_stereo\": len(stereo_set)\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Calculate the number of unique values for each property\n",
    "unique_properties = count_unique_attributes(cleaned_data)\n",
    "print(unique_properties)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93f9cc5bc7439509",
    "outputId": "7a812db8-6392-4018-9891-8ec3de68014c",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:27:31.784809Z",
     "start_time": "2024-05-08T06:27:31.710701Z"
    }
   },
   "id": "93f9cc5bc7439509",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a Dataset class\n",
    "class MolecularGraphDataset(Dataset):\n",
    "    def __init__(self, cleaned_data, transform=None, pre_transform=None):\n",
    "        super(MolecularGraphDataset, self).__init__(transform=transform, pre_transform=pre_transform)\n",
    "        self.graphs = list(cleaned_data.values())\n",
    "        self._indices = range(len(self.graphs))\n",
    "        self.targets = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._indices)\n",
    "\n",
    "    def get(self, idx):\n",
    "        graph_info = self.graphs[idx]\n",
    "        return self.create_pyg_data(graph_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.get(self._indices[idx])\n",
    "        data = data if self.transform is None else self.transform(data)\n",
    "        return data\n",
    "\n",
    "    def extract_targets(self):\n",
    "        for graph_info in self.graphs:\n",
    "            for node_id, node_info in graph_info[\"target_variable\"].items():\n",
    "                self.targets.append([\n",
    "                    node_info[\"mass\"],\n",
    "                    node_info[\"charge\"],\n",
    "                    node_info[\"sigma\"],\n",
    "                    node_info[\"epsilon\"]\n",
    "                ])\n",
    "        return self.targets\n",
    "\n",
    "    def create_pyg_data(self, graph_info):\n",
    "        # Extract nodes and edges from the graph information\n",
    "        node_id_feature = graph_info[\"node_id_feature\"]\n",
    "        edge_features = graph_info[\"edge_features\"]\n",
    "        target_variable = graph_info[\"target_variable\"]\n",
    "\n",
    "        # Create the node feature matrix\n",
    "        node_ids = sorted(node_id_feature.keys())\n",
    "        node_features = []\n",
    "        for node_id in node_ids:\n",
    "            features = [\n",
    "                node_id_feature[node_id][\"atomic\"],\n",
    "                node_id_feature[node_id][\"valence\"],\n",
    "                node_id_feature[node_id][\"formal_charge\"],\n",
    "                node_id_feature[node_id][\"aromatic\"],\n",
    "                node_id_feature[node_id][\"hybridization\"],\n",
    "                node_id_feature[node_id][\"radical_electrons\"]\n",
    "            ]\n",
    "            node_features.append(features)\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "        # Create the edge list\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        for edge in edge_features:\n",
    "            edge_index.append([edge[\"source\"], edge[\"target\"]])\n",
    "            edge_attr.append([\n",
    "                edge[\"type\"],\n",
    "                edge[\"stereo\"],\n",
    "                edge[\"aromatic\"],\n",
    "                edge[\"conjugated\"]\n",
    "            ])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "        # Create the target variable tensor\n",
    "        target_list = [target_variable[node_id] for node_id in node_ids]\n",
    "        y = torch.tensor([[t[\"mass\"], t[\"charge\"], t[\"sigma\"], t[\"epsilon\"]] for t in target_list], dtype=torch.float)\n",
    "\n",
    "        #mean = torch.mean(y, dim=0)\n",
    "        #std = torch.std(y, dim=0)\n",
    "\n",
    "        # Standardize the target variables\n",
    "        #y = (y - mean) / std\n",
    "\n",
    "        # Return the graph as a Data object\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)"
   ],
   "metadata": {
    "id": "ff1ef6cc61d6b719",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:27:32.284426Z",
     "start_time": "2024-05-08T06:27:32.272846Z"
    }
   },
   "id": "ff1ef6cc61d6b719",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = MolecularGraphDataset(cleaned_data)"
   ],
   "metadata": {
    "id": "846d51633e0cc2e4",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:27:33.281700Z",
     "start_time": "2024-05-08T06:27:33.277887Z"
    }
   },
   "id": "846d51633e0cc2e4",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "def apply_random_mask(mol_graph, p, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    N = mol_graph.x.size(0)\n",
    "    num_mask_nodes = max(1, math.floor(p * N))\n",
    "    mask_nodes = random.sample(list(range(N)), num_mask_nodes)\n",
    "\n",
    "    aug_mol_graph = deepcopy(mol_graph)\n",
    "    for atom_idx in mask_nodes:\n",
    "        aug_mol_graph.x[atom_idx, :] = torch.zeros(6)\n",
    "\n",
    "    return aug_mol_graph\n",
    "\n",
    "def apply_random_bond_deletion(mol_graph, p, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    M = mol_graph.edge_index.size(1) // 2\n",
    "    num_mask_edges = max([0, math.floor(p * M)])\n",
    "    mask_edges_single = random.sample(list(range(M)), num_mask_edges)\n",
    "    mask_edges = [2*i for i in mask_edges_single] + [2*i+1 for i in mask_edges_single]\n",
    "\n",
    "    aug_mol_graph = deepcopy(mol_graph)\n",
    "\n",
    "    num_features_per_edge = mol_graph.edge_attr.size(1)\n",
    "    aug_mol_graph.edge_index = torch.zeros((2, 2 * (M - num_mask_edges)))\n",
    "    aug_mol_graph.edge_attr = torch.zeros((2 * (M - num_mask_edges), num_features_per_edge))\n",
    "    count = 0\n",
    "    for bond_idx in range(2 * M):\n",
    "        if bond_idx not in mask_edges:\n",
    "            aug_mol_graph.edge_index[:, count] = mol_graph.edge_index[:, bond_idx]\n",
    "            aug_mol_graph.edge_attr[count, :] = mol_graph.edge_attr[bond_idx, :]\n",
    "            count += 1\n",
    "\n",
    "    return aug_mol_graph"
   ],
   "metadata": {
    "id": "0RXUc7O537PJ",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:27:36.696915Z",
     "start_time": "2024-05-08T06:27:36.688708Z"
    }
   },
   "id": "0RXUc7O537PJ",
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NodeEmbedding(nn.Module):\n",
    "    def __init__(self, num_atomic, num_valence, num_formal_charge, num_hybridization, num_radical_electrons, embedding_dim):\n",
    "        super(NodeEmbedding, self).__init__()\n",
    "        self.atomic_embedding = nn.Embedding(num_atomic, embedding_dim)\n",
    "        self.valence_embedding = nn.Embedding(num_valence, embedding_dim)\n",
    "        self.formal_charge_embedding = nn.Embedding(num_formal_charge, embedding_dim)\n",
    "        self.hybridization_embedding = nn.Embedding(num_hybridization, embedding_dim)\n",
    "        self.radical_electrons_embedding = nn.Embedding(num_radical_electrons, embedding_dim)\n",
    "\n",
    "    def forward(self, atomic, valence, formal_charge, aromatic, hybridization, radical_electrons):\n",
    "        atomic_embed = self.atomic_embedding(atomic)\n",
    "        valence_embed = self.valence_embedding(valence)\n",
    "        formal_charge_embed = self.formal_charge_embedding(formal_charge)\n",
    "        hybridization_embed = self.hybridization_embedding(hybridization)\n",
    "        radical_electrons_embed = self.radical_electrons_embedding(radical_electrons)\n",
    "\n",
    "        # Concatenate boolean features\n",
    "        other_features = torch.stack([aromatic], dim=1).float()\n",
    "\n",
    "        # Concatenate all features together\n",
    "        return torch.cat([atomic_embed, valence_embed, formal_charge_embed, hybridization_embed, radical_electrons_embed, other_features], dim=1)\n",
    "\n",
    "class EdgeEmbedding(nn.Module):\n",
    "    def __init__(self, num_type, num_stereo, embedding_dim):\n",
    "        super(EdgeEmbedding, self).__init__()\n",
    "        self.type_embedding = nn.Embedding(num_type, embedding_dim)\n",
    "        self.stereo_embedding = nn.Embedding(num_stereo, embedding_dim)\n",
    "\n",
    "    def forward(self, type_, stereo, aromatic, conjugated):\n",
    "        type_embed = self.type_embedding(type_)\n",
    "        stereo_embed = self.stereo_embedding(stereo)\n",
    "\n",
    "        # Concatenate boolean features directly\n",
    "        other_features = torch.stack([aromatic, conjugated], dim=1).float()\n",
    "\n",
    "        # Concatenate all features together\n",
    "        return torch.cat([type_embed, stereo_embed, other_features], dim=1)"
   ],
   "metadata": {
    "id": "3b7d0c1dec527eb7",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:27:36.946915Z",
     "start_time": "2024-05-08T06:27:36.937045Z"
    }
   },
   "id": "3b7d0c1dec527eb7",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ImprovedGNNWithEmbeddings(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, node_embedding_dim, edge_embedding_dim, hidden_dim, output_dim, num_layers = 6, num_atomic = 12, num_valence = 7, num_hybridization = 5, num_type = 4, num_stereo = 3 ,num_formal_charge = 3, num_radical_electrons = 1):\n",
    "        super(ImprovedGNNWithEmbeddings, self).__init__()\n",
    "        self.node_embedding = NodeEmbedding(num_atomic, num_valence, num_formal_charge, num_hybridization, num_radical_electrons, node_embedding_dim)\n",
    "        self.edge_embedding = EdgeEmbedding(num_type, num_stereo, edge_embedding_dim)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Define the first GINEConv layer, with the correct edge_dim specified\n",
    "        self.convs.append(GINEConv(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(node_input_dim, hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "            ),\n",
    "            edge_dim=edge_input_dim\n",
    "        ))\n",
    "        self.norms.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        # Additional GINEConv layers, each with the correct edge_dim\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GINEConv(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "                ),\n",
    "                edge_dim=edge_input_dim\n",
    "            ))\n",
    "            self.norms.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        # Jumping Knowledge mechanism\n",
    "        self.jump = JumpingKnowledge(mode=\"cat\")\n",
    "\n",
    "        # Final fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim * num_layers, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        features = []\n",
    "\n",
    "        # Pass through GINEConv layers and apply batch normalization\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = F.relu(norm(x))\n",
    "            features.append(x)\n",
    "\n",
    "        # Apply Jumping Knowledge (JK) to concatenate all layers\n",
    "        x = self.jump(features)\n",
    "\n",
    "        # Directly pass through the linear layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "302b42750e77409d",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:27:37.359022Z",
     "start_time": "2024-05-08T06:27:37.349019Z"
    }
   },
   "id": "302b42750e77409d",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ImprovedGNNWithEmbeddings(\n  (node_embedding): NodeEmbedding(\n    (atomic_embedding): Embedding(12, 32)\n    (valence_embedding): Embedding(7, 32)\n    (formal_charge_embedding): Embedding(3, 32)\n    (hybridization_embedding): Embedding(5, 32)\n    (radical_electrons_embedding): Embedding(1, 32)\n  )\n  (edge_embedding): EdgeEmbedding(\n    (type_embedding): Embedding(4, 32)\n    (stereo_embedding): Embedding(3, 32)\n  )\n  (convs): ModuleList(\n    (0): GINEConv(nn=Sequential(\n      (0): Linear(in_features=6, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n    ))\n    (1-5): 5 x GINEConv(nn=Sequential(\n      (0): Linear(in_features=256, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n    ))\n  )\n  (norms): ModuleList(\n    (0-5): 6 x BatchNorm(256)\n  )\n  (jump): JumpingKnowledge(cat)\n  (fc1): Linear(in_features=1536, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=4, bias=True)\n)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "node_input_dim = 6  # node feature dimension\n",
    "edge_input_dim = 4  # edge feature dimension\n",
    "hidden_dim = 128\n",
    "output_dim = 4  # The number of outputs (mass, charge, sigma, epsilon)\n",
    "num_epochs = 500\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "# model = ImprovedGNNWithEdgeFeatures(node_input_dim, edge_input_dim, hidden_dim, output_dim)\n",
    "model = ImprovedGNNWithEmbeddings( node_embedding_dim = 32, edge_embedding_dim = 32, hidden_dim = 256, output_dim = 4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cfbb21124a7fd29",
    "outputId": "287362d8-8c23-4ba6-bc29-341180579f6d",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:33:10.702233Z",
     "start_time": "2024-05-08T06:33:10.678425Z"
    }
   },
   "id": "4cfbb21124a7fd29",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "source": [
    "def unstandardize(targets, mean, std):\n",
    "    return targets * std + mean\n",
    "\n",
    "mean = torch.tensor(mean).to(device)\n",
    "std = torch.tensor(std).to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "goyNlLrxF_tL",
    "outputId": "63a991a9-d634-4dd7-88e9-0f3b0d701623",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:11:26.722810Z",
     "start_time": "2024-05-08T06:11:26.702219Z"
    }
   },
   "id": "goyNlLrxF_tL",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munstandardize\u001B[39m(targets, mean, std):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m targets \u001B[38;5;241m*\u001B[39m std \u001B[38;5;241m+\u001B[39m mean\n\u001B[0;32m----> 4\u001B[0m mean \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[43mmean\u001B[49m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      5\u001B[0m std \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(std)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mean' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 19.0993, Val Loss: 15.6042\n",
      "Epoch: 002, Train Loss: 7.2428, Val Loss: 8.0296\n",
      "Epoch: 003, Train Loss: 1.9892, Val Loss: 2.7965\n",
      "Epoch: 004, Train Loss: 0.7506, Val Loss: 0.6464\n",
      "Epoch: 005, Train Loss: 0.4129, Val Loss: 0.5097\n",
      "Epoch: 006, Train Loss: 0.2544, Val Loss: 0.2481\n",
      "Epoch: 007, Train Loss: 0.1878, Val Loss: 0.2984\n",
      "Epoch: 008, Train Loss: 0.1397, Val Loss: 0.3997\n",
      "Epoch: 009, Train Loss: 0.1658, Val Loss: 0.2279\n",
      "Epoch: 010, Train Loss: 0.1407, Val Loss: 0.3722\n",
      "Epoch: 011, Train Loss: 0.1456, Val Loss: 0.1975\n",
      "Epoch: 012, Train Loss: 0.0969, Val Loss: 0.0953\n",
      "Epoch: 013, Train Loss: 0.0980, Val Loss: 0.1153\n",
      "Epoch: 014, Train Loss: 0.0778, Val Loss: 0.0777\n",
      "Epoch: 015, Train Loss: 0.0584, Val Loss: 0.0543\n",
      "Epoch: 016, Train Loss: 0.0679, Val Loss: 0.0871\n",
      "Epoch: 017, Train Loss: 0.0734, Val Loss: 0.0735\n",
      "Epoch: 018, Train Loss: 0.0534, Val Loss: 0.2327\n",
      "Epoch: 019, Train Loss: 0.0590, Val Loss: 0.0736\n",
      "Epoch: 020, Train Loss: 0.0588, Val Loss: 0.0427\n",
      "Epoch: 021, Train Loss: 0.0528, Val Loss: 0.0626\n",
      "Epoch: 022, Train Loss: 0.0468, Val Loss: 0.0957\n",
      "Epoch: 023, Train Loss: 0.0373, Val Loss: 0.0819\n",
      "Epoch: 024, Train Loss: 0.0460, Val Loss: 0.0442\n",
      "Epoch: 025, Train Loss: 0.0360, Val Loss: 0.0412\n",
      "Epoch: 026, Train Loss: 0.0391, Val Loss: 0.0427\n",
      "Epoch: 027, Train Loss: 0.0362, Val Loss: 0.0367\n",
      "Epoch: 028, Train Loss: 0.0304, Val Loss: 0.0422\n",
      "Epoch: 029, Train Loss: 0.0225, Val Loss: 0.0505\n",
      "Epoch: 030, Train Loss: 0.0265, Val Loss: 0.0308\n",
      "Epoch: 031, Train Loss: 0.0286, Val Loss: 0.0281\n",
      "Epoch: 032, Train Loss: 0.0285, Val Loss: 0.0381\n",
      "Epoch: 033, Train Loss: 0.0251, Val Loss: 0.0241\n",
      "Epoch: 034, Train Loss: 0.0300, Val Loss: 0.0417\n",
      "Epoch: 035, Train Loss: 0.0207, Val Loss: 0.0273\n",
      "Epoch: 036, Train Loss: 0.0192, Val Loss: 0.0592\n",
      "Epoch: 037, Train Loss: 0.0256, Val Loss: 0.0442\n",
      "Epoch: 038, Train Loss: 0.0235, Val Loss: 0.0282\n",
      "Epoch: 039, Train Loss: 0.0198, Val Loss: 0.0233\n",
      "Epoch: 040, Train Loss: 0.0224, Val Loss: 0.0209\n",
      "Epoch: 041, Train Loss: 0.0182, Val Loss: 0.0291\n",
      "Epoch: 042, Train Loss: 0.0210, Val Loss: 0.0420\n",
      "Epoch: 043, Train Loss: 0.0184, Val Loss: 0.0245\n",
      "Epoch: 044, Train Loss: 0.0174, Val Loss: 0.0222\n",
      "Epoch: 045, Train Loss: 0.0199, Val Loss: 0.0305\n",
      "Epoch: 046, Train Loss: 0.0153, Val Loss: 0.0346\n",
      "Epoch: 047, Train Loss: 0.0223, Val Loss: 0.0270\n",
      "Epoch: 048, Train Loss: 0.0181, Val Loss: 0.0428\n",
      "Epoch: 049, Train Loss: 0.0304, Val Loss: 0.0245\n",
      "Epoch: 050, Train Loss: 0.0249, Val Loss: 0.0272\n",
      "Epoch: 051, Train Loss: 0.0215, Val Loss: 0.0228\n",
      "Epoch: 052, Train Loss: 0.0255, Val Loss: 0.0428\n",
      "Epoch: 053, Train Loss: 0.0305, Val Loss: 0.0259\n",
      "Epoch: 054, Train Loss: 0.0164, Val Loss: 0.0218\n",
      "Epoch: 055, Train Loss: 0.0151, Val Loss: 0.0269\n",
      "Epoch: 056, Train Loss: 0.0182, Val Loss: 0.0173\n",
      "Epoch: 057, Train Loss: 0.0116, Val Loss: 0.0218\n",
      "Epoch: 058, Train Loss: 0.0137, Val Loss: 0.0196\n",
      "Epoch: 059, Train Loss: 0.0183, Val Loss: 0.0198\n",
      "Epoch: 060, Train Loss: 0.0145, Val Loss: 0.0208\n",
      "Epoch: 061, Train Loss: 0.0156, Val Loss: 0.0202\n",
      "Epoch: 062, Train Loss: 0.0151, Val Loss: 0.0224\n",
      "Epoch: 063, Train Loss: 0.0147, Val Loss: 0.0211\n",
      "Epoch: 064, Train Loss: 0.0151, Val Loss: 0.0171\n",
      "Epoch: 065, Train Loss: 0.0134, Val Loss: 0.0224\n",
      "Epoch: 066, Train Loss: 0.0136, Val Loss: 0.0158\n",
      "Epoch: 067, Train Loss: 0.0143, Val Loss: 0.0174\n",
      "Epoch: 068, Train Loss: 0.0152, Val Loss: 0.0141\n",
      "Epoch: 069, Train Loss: 0.0130, Val Loss: 0.0213\n",
      "Epoch: 070, Train Loss: 0.0146, Val Loss: 0.0158\n",
      "Epoch: 071, Train Loss: 0.0124, Val Loss: 0.0172\n",
      "Epoch: 072, Train Loss: 0.0152, Val Loss: 0.0225\n",
      "Epoch: 073, Train Loss: 0.0133, Val Loss: 0.0197\n",
      "Epoch: 074, Train Loss: 0.0117, Val Loss: 0.0156\n",
      "Epoch: 075, Train Loss: 0.0119, Val Loss: 0.0193\n",
      "Epoch: 076, Train Loss: 0.0151, Val Loss: 0.0218\n",
      "Epoch: 077, Train Loss: 0.0124, Val Loss: 0.0225\n",
      "Epoch: 078, Train Loss: 0.0120, Val Loss: 0.0245\n",
      "Epoch: 079, Train Loss: 0.0135, Val Loss: 0.0171\n",
      "Epoch: 080, Train Loss: 0.0113, Val Loss: 0.0170\n",
      "Epoch: 081, Train Loss: 0.0139, Val Loss: 0.0192\n",
      "Epoch: 082, Train Loss: 0.0119, Val Loss: 0.0172\n",
      "Epoch: 083, Train Loss: 0.0096, Val Loss: 0.0152\n",
      "Epoch: 084, Train Loss: 0.0112, Val Loss: 0.0138\n",
      "Epoch: 085, Train Loss: 0.0113, Val Loss: 0.0163\n",
      "Epoch: 086, Train Loss: 0.0134, Val Loss: 0.0155\n",
      "Epoch: 087, Train Loss: 0.0112, Val Loss: 0.0144\n",
      "Epoch: 088, Train Loss: 0.0125, Val Loss: 0.0184\n",
      "Epoch: 089, Train Loss: 0.0114, Val Loss: 0.0119\n",
      "Epoch: 090, Train Loss: 0.0106, Val Loss: 0.0219\n",
      "Epoch: 091, Train Loss: 0.0240, Val Loss: 0.0561\n",
      "Epoch: 092, Train Loss: 0.0316, Val Loss: 0.0421\n",
      "Epoch: 093, Train Loss: 0.0254, Val Loss: 0.0232\n",
      "Epoch: 094, Train Loss: 0.0131, Val Loss: 0.0153\n",
      "Epoch: 095, Train Loss: 0.0112, Val Loss: 0.0130\n",
      "Epoch: 096, Train Loss: 0.0103, Val Loss: 0.0122\n",
      "Epoch: 097, Train Loss: 0.0089, Val Loss: 0.0146\n",
      "Epoch: 098, Train Loss: 0.0098, Val Loss: 0.0137\n",
      "Epoch: 099, Train Loss: 0.0090, Val Loss: 0.0159\n",
      "Epoch: 100, Train Loss: 0.0074, Val Loss: 0.0107\n",
      "Epoch: 101, Train Loss: 0.0094, Val Loss: 0.0107\n",
      "Epoch: 102, Train Loss: 0.0088, Val Loss: 0.0103\n",
      "Epoch: 103, Train Loss: 0.0074, Val Loss: 0.0106\n",
      "Epoch: 104, Train Loss: 0.0090, Val Loss: 0.0131\n",
      "Epoch: 105, Train Loss: 0.0098, Val Loss: 0.0186\n",
      "Epoch: 106, Train Loss: 0.0078, Val Loss: 0.0123\n",
      "Epoch: 107, Train Loss: 0.0161, Val Loss: 0.0182\n",
      "Epoch: 108, Train Loss: 0.0130, Val Loss: 0.0168\n",
      "Epoch: 109, Train Loss: 0.0106, Val Loss: 0.0118\n",
      "Epoch: 110, Train Loss: 0.0116, Val Loss: 0.0122\n",
      "Epoch: 111, Train Loss: 0.0111, Val Loss: 0.0186\n",
      "Epoch: 112, Train Loss: 0.0089, Val Loss: 0.0100\n",
      "Epoch: 113, Train Loss: 0.0070, Val Loss: 0.0101\n",
      "Epoch: 114, Train Loss: 0.0079, Val Loss: 0.0132\n",
      "Epoch: 115, Train Loss: 0.0096, Val Loss: 0.0150\n",
      "Epoch: 116, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch: 117, Train Loss: 0.0080, Val Loss: 0.0148\n",
      "Epoch: 118, Train Loss: 0.0120, Val Loss: 0.0190\n",
      "Epoch: 119, Train Loss: 0.0122, Val Loss: 0.0165\n",
      "Epoch: 120, Train Loss: 0.0095, Val Loss: 0.0160\n",
      "Epoch: 121, Train Loss: 0.0094, Val Loss: 0.0129\n",
      "Epoch: 122, Train Loss: 0.0083, Val Loss: 0.0104\n",
      "Epoch: 123, Train Loss: 0.0082, Val Loss: 0.0094\n",
      "Epoch: 124, Train Loss: 0.0086, Val Loss: 0.0107\n",
      "Epoch: 125, Train Loss: 0.0068, Val Loss: 0.0086\n",
      "Epoch: 126, Train Loss: 0.0058, Val Loss: 0.0102\n",
      "Epoch: 127, Train Loss: 0.0076, Val Loss: 0.0154\n",
      "Epoch: 128, Train Loss: 0.0072, Val Loss: 0.0126\n",
      "Epoch: 129, Train Loss: 0.0099, Val Loss: 0.0141\n",
      "Epoch: 130, Train Loss: 0.0078, Val Loss: 0.0155\n",
      "Epoch: 131, Train Loss: 0.0084, Val Loss: 0.0106\n",
      "Epoch: 132, Train Loss: 0.0106, Val Loss: 0.0108\n",
      "Epoch: 133, Train Loss: 0.0080, Val Loss: 0.0093\n",
      "Epoch: 134, Train Loss: 0.0071, Val Loss: 0.0109\n",
      "Epoch: 135, Train Loss: 0.0075, Val Loss: 0.0116\n",
      "Epoch: 136, Train Loss: 0.0061, Val Loss: 0.0090\n",
      "Epoch: 137, Train Loss: 0.0055, Val Loss: 0.0095\n",
      "Epoch: 138, Train Loss: 0.0085, Val Loss: 0.0151\n",
      "Epoch: 139, Train Loss: 0.0094, Val Loss: 0.0224\n",
      "Epoch: 140, Train Loss: 0.0094, Val Loss: 0.0157\n",
      "Epoch: 141, Train Loss: 0.0085, Val Loss: 0.0172\n",
      "Epoch: 142, Train Loss: 0.0086, Val Loss: 0.0094\n",
      "Epoch: 143, Train Loss: 0.0074, Val Loss: 0.0102\n",
      "Epoch: 144, Train Loss: 0.0103, Val Loss: 0.0167\n",
      "Epoch: 145, Train Loss: 0.0103, Val Loss: 0.0112\n",
      "Epoch: 146, Train Loss: 0.0095, Val Loss: 0.0300\n",
      "Epoch: 147, Train Loss: 0.0070, Val Loss: 0.0093\n",
      "Epoch: 148, Train Loss: 0.0076, Val Loss: 0.0100\n",
      "Epoch: 149, Train Loss: 0.0070, Val Loss: 0.0099\n",
      "Epoch: 150, Train Loss: 0.0070, Val Loss: 0.0094\n",
      "Epoch: 151, Train Loss: 0.0061, Val Loss: 0.0112\n",
      "Epoch: 152, Train Loss: 0.0085, Val Loss: 0.0099\n",
      "Epoch: 153, Train Loss: 0.0099, Val Loss: 0.0152\n",
      "Epoch: 154, Train Loss: 0.0117, Val Loss: 0.0128\n",
      "Epoch: 155, Train Loss: 0.0080, Val Loss: 0.0118\n",
      "Epoch: 156, Train Loss: 0.0062, Val Loss: 0.0101\n",
      "Epoch: 157, Train Loss: 0.0053, Val Loss: 0.0093\n",
      "Epoch: 158, Train Loss: 0.0059, Val Loss: 0.0110\n",
      "Epoch: 159, Train Loss: 0.0077, Val Loss: 0.0084\n",
      "Epoch: 160, Train Loss: 0.0068, Val Loss: 0.0091\n",
      "Epoch: 161, Train Loss: 0.0058, Val Loss: 0.0119\n",
      "Epoch: 162, Train Loss: 0.0079, Val Loss: 0.0097\n",
      "Epoch: 163, Train Loss: 0.0104, Val Loss: 0.0118\n",
      "Epoch: 164, Train Loss: 0.0100, Val Loss: 0.0109\n",
      "Epoch: 165, Train Loss: 0.0107, Val Loss: 0.0108\n",
      "Epoch: 166, Train Loss: 0.0078, Val Loss: 0.0100\n",
      "Epoch: 167, Train Loss: 0.0066, Val Loss: 0.0087\n",
      "Epoch: 168, Train Loss: 0.0063, Val Loss: 0.0113\n",
      "Epoch: 169, Train Loss: 0.0077, Val Loss: 0.0099\n",
      "Epoch: 170, Train Loss: 0.0071, Val Loss: 0.0116\n",
      "Epoch: 171, Train Loss: 0.0097, Val Loss: 0.0108\n",
      "Epoch: 172, Train Loss: 0.0061, Val Loss: 0.0096\n",
      "Epoch: 173, Train Loss: 0.0069, Val Loss: 0.0091\n",
      "Epoch: 174, Train Loss: 0.0078, Val Loss: 0.0156\n",
      "Epoch: 175, Train Loss: 0.0078, Val Loss: 0.0133\n",
      "Epoch: 176, Train Loss: 0.0082, Val Loss: 0.0087\n",
      "Epoch: 177, Train Loss: 0.0058, Val Loss: 0.0130\n",
      "Epoch: 178, Train Loss: 0.0054, Val Loss: 0.0091\n",
      "Epoch: 179, Train Loss: 0.0075, Val Loss: 0.0232\n",
      "Epoch: 180, Train Loss: 0.0080, Val Loss: 0.0157\n",
      "Epoch: 181, Train Loss: 0.0057, Val Loss: 0.0100\n",
      "Epoch: 182, Train Loss: 0.0084, Val Loss: 0.0128\n",
      "Epoch: 183, Train Loss: 0.0083, Val Loss: 0.0094\n",
      "Epoch: 184, Train Loss: 0.0054, Val Loss: 0.0074\n",
      "Epoch: 185, Train Loss: 0.0060, Val Loss: 0.0106\n",
      "Epoch: 186, Train Loss: 0.0065, Val Loss: 0.0129\n",
      "Epoch: 187, Train Loss: 0.0075, Val Loss: 0.0134\n",
      "Epoch: 188, Train Loss: 0.0075, Val Loss: 0.0099\n",
      "Epoch: 189, Train Loss: 0.0066, Val Loss: 0.0225\n",
      "Epoch: 190, Train Loss: 0.0061, Val Loss: 0.0091\n",
      "Epoch: 191, Train Loss: 0.0067, Val Loss: 0.0165\n",
      "Epoch: 192, Train Loss: 0.0073, Val Loss: 0.0102\n",
      "Epoch: 193, Train Loss: 0.0091, Val Loss: 0.0120\n",
      "Epoch: 194, Train Loss: 0.0072, Val Loss: 0.0091\n",
      "Epoch: 195, Train Loss: 0.0056, Val Loss: 0.0149\n",
      "Epoch: 196, Train Loss: 0.0059, Val Loss: 0.0077\n",
      "Epoch: 197, Train Loss: 0.0060, Val Loss: 0.0093\n",
      "Epoch: 198, Train Loss: 0.0058, Val Loss: 0.0084\n",
      "Epoch: 199, Train Loss: 0.0056, Val Loss: 0.0084\n",
      "Epoch: 200, Train Loss: 0.0065, Val Loss: 0.0078\n",
      "Epoch: 201, Train Loss: 0.0062, Val Loss: 0.0080\n",
      "Epoch: 202, Train Loss: 0.0105, Val Loss: 0.0150\n",
      "Epoch: 203, Train Loss: 0.0101, Val Loss: 0.0153\n",
      "Epoch: 204, Train Loss: 0.0083, Val Loss: 0.0095\n",
      "Epoch: 205, Train Loss: 0.0098, Val Loss: 0.0164\n",
      "Epoch: 206, Train Loss: 0.0082, Val Loss: 0.0118\n",
      "Epoch: 207, Train Loss: 0.0082, Val Loss: 0.0135\n",
      "Epoch: 208, Train Loss: 0.0101, Val Loss: 0.0206\n",
      "Epoch: 209, Train Loss: 0.0088, Val Loss: 0.0127\n",
      "Epoch: 210, Train Loss: 0.0094, Val Loss: 0.0106\n",
      "Epoch: 211, Train Loss: 0.0071, Val Loss: 0.0119\n",
      "Epoch: 212, Train Loss: 0.0090, Val Loss: 0.0117\n",
      "Epoch: 213, Train Loss: 0.0090, Val Loss: 0.0097\n",
      "Epoch: 214, Train Loss: 0.0062, Val Loss: 0.0101\n",
      "Epoch: 215, Train Loss: 0.0061, Val Loss: 0.0139\n",
      "Epoch: 216, Train Loss: 0.0094, Val Loss: 0.0113\n",
      "Epoch: 217, Train Loss: 0.0082, Val Loss: 0.0136\n",
      "Epoch: 218, Train Loss: 0.0078, Val Loss: 0.0156\n",
      "Epoch: 219, Train Loss: 0.0141, Val Loss: 0.0314\n",
      "Epoch: 220, Train Loss: 0.0122, Val Loss: 0.0152\n",
      "Epoch: 221, Train Loss: 0.0120, Val Loss: 0.0160\n",
      "Epoch: 222, Train Loss: 0.0106, Val Loss: 0.0164\n",
      "Epoch: 223, Train Loss: 0.0087, Val Loss: 0.0134\n",
      "Epoch: 224, Train Loss: 0.0078, Val Loss: 0.0127\n",
      "Epoch: 225, Train Loss: 0.0077, Val Loss: 0.0104\n",
      "Epoch: 226, Train Loss: 0.0084, Val Loss: 0.0101\n",
      "Epoch: 227, Train Loss: 0.0064, Val Loss: 0.0103\n",
      "Epoch: 228, Train Loss: 0.0059, Val Loss: 0.0072\n",
      "Epoch: 229, Train Loss: 0.0045, Val Loss: 0.0128\n",
      "Epoch: 230, Train Loss: 0.0052, Val Loss: 0.0096\n",
      "Epoch: 231, Train Loss: 0.0056, Val Loss: 0.0082\n",
      "Epoch: 232, Train Loss: 0.0060, Val Loss: 0.0075\n",
      "Epoch: 233, Train Loss: 0.0057, Val Loss: 0.0074\n",
      "Epoch: 234, Train Loss: 0.0054, Val Loss: 0.0073\n",
      "Epoch: 235, Train Loss: 0.0058, Val Loss: 0.0111\n",
      "Epoch: 236, Train Loss: 0.0119, Val Loss: 0.0228\n",
      "Epoch: 237, Train Loss: 0.0071, Val Loss: 0.0083\n",
      "Epoch: 238, Train Loss: 0.0071, Val Loss: 0.0090\n",
      "Epoch: 239, Train Loss: 0.0047, Val Loss: 0.0103\n",
      "Epoch: 240, Train Loss: 0.0049, Val Loss: 0.0093\n",
      "Epoch: 241, Train Loss: 0.0060, Val Loss: 0.0088\n",
      "Epoch: 242, Train Loss: 0.0072, Val Loss: 0.0085\n",
      "Epoch: 243, Train Loss: 0.0070, Val Loss: 0.0125\n",
      "Epoch: 244, Train Loss: 0.0105, Val Loss: 0.0116\n",
      "Epoch: 245, Train Loss: 0.0102, Val Loss: 0.0132\n",
      "Epoch: 246, Train Loss: 0.0076, Val Loss: 0.0087\n",
      "Epoch: 247, Train Loss: 0.0060, Val Loss: 0.0098\n",
      "Epoch: 248, Train Loss: 0.0054, Val Loss: 0.0091\n",
      "Epoch: 249, Train Loss: 0.0087, Val Loss: 0.0113\n",
      "Epoch: 250, Train Loss: 0.0066, Val Loss: 0.0076\n",
      "Epoch: 251, Train Loss: 0.0049, Val Loss: 0.0113\n",
      "Epoch: 252, Train Loss: 0.0083, Val Loss: 0.0105\n",
      "Epoch: 253, Train Loss: 0.0050, Val Loss: 0.0083\n",
      "Epoch: 254, Train Loss: 0.0068, Val Loss: 0.0076\n",
      "Epoch: 255, Train Loss: 0.0047, Val Loss: 0.0094\n",
      "Epoch: 256, Train Loss: 0.0066, Val Loss: 0.0113\n",
      "Epoch: 257, Train Loss: 0.0088, Val Loss: 0.0101\n",
      "Epoch: 258, Train Loss: 0.0073, Val Loss: 0.0126\n",
      "Epoch: 259, Train Loss: 0.0065, Val Loss: 0.0110\n",
      "Epoch: 260, Train Loss: 0.0081, Val Loss: 0.0097\n",
      "Epoch: 261, Train Loss: 0.0099, Val Loss: 0.0179\n",
      "Epoch: 262, Train Loss: 0.0118, Val Loss: 0.0181\n",
      "Epoch: 263, Train Loss: 0.0157, Val Loss: 0.0106\n",
      "Epoch: 264, Train Loss: 0.0075, Val Loss: 0.0109\n",
      "Epoch: 265, Train Loss: 0.0055, Val Loss: 0.0080\n",
      "Epoch: 266, Train Loss: 0.0044, Val Loss: 0.0103\n",
      "Epoch: 267, Train Loss: 0.0056, Val Loss: 0.0125\n",
      "Epoch: 268, Train Loss: 0.0055, Val Loss: 0.0081\n",
      "Epoch: 269, Train Loss: 0.0053, Val Loss: 0.0080\n",
      "Epoch: 270, Train Loss: 0.0049, Val Loss: 0.0075\n",
      "Epoch: 271, Train Loss: 0.0063, Val Loss: 0.0120\n",
      "Epoch: 272, Train Loss: 0.0050, Val Loss: 0.0072\n",
      "Epoch: 273, Train Loss: 0.0057, Val Loss: 0.0107\n",
      "Epoch: 274, Train Loss: 0.0111, Val Loss: 0.0355\n",
      "Epoch: 275, Train Loss: 0.0127, Val Loss: 0.0309\n",
      "Epoch: 276, Train Loss: 0.0070, Val Loss: 0.0075\n",
      "Epoch: 277, Train Loss: 0.0058, Val Loss: 0.0107\n",
      "Epoch: 278, Train Loss: 0.0066, Val Loss: 0.0083\n",
      "Epoch: 279, Train Loss: 0.0059, Val Loss: 0.0103\n",
      "Epoch: 280, Train Loss: 0.0054, Val Loss: 0.0079\n",
      "Epoch: 281, Train Loss: 0.0050, Val Loss: 0.0077\n",
      "Epoch: 282, Train Loss: 0.0053, Val Loss: 0.0086\n",
      "Epoch: 283, Train Loss: 0.0057, Val Loss: 0.0077\n",
      "Epoch: 284, Train Loss: 0.0050, Val Loss: 0.0078\n",
      "Epoch: 285, Train Loss: 0.0059, Val Loss: 0.0090\n",
      "Epoch: 286, Train Loss: 0.0058, Val Loss: 0.0076\n",
      "Epoch: 287, Train Loss: 0.0055, Val Loss: 0.0071\n",
      "Epoch: 288, Train Loss: 0.0063, Val Loss: 0.0158\n",
      "Epoch: 289, Train Loss: 0.0082, Val Loss: 0.0093\n",
      "Epoch: 290, Train Loss: 0.0047, Val Loss: 0.0082\n",
      "Epoch: 291, Train Loss: 0.0043, Val Loss: 0.0071\n",
      "Epoch: 292, Train Loss: 0.0064, Val Loss: 0.0083\n",
      "Epoch: 293, Train Loss: 0.0068, Val Loss: 0.0112\n",
      "Epoch: 294, Train Loss: 0.0049, Val Loss: 0.0093\n",
      "Epoch: 295, Train Loss: 0.0052, Val Loss: 0.0083\n",
      "Epoch: 296, Train Loss: 0.0058, Val Loss: 0.0098\n",
      "Epoch: 297, Train Loss: 0.0045, Val Loss: 0.0062\n",
      "Epoch: 298, Train Loss: 0.0061, Val Loss: 0.0081\n",
      "Epoch: 299, Train Loss: 0.0057, Val Loss: 0.0069\n",
      "Epoch: 300, Train Loss: 0.0055, Val Loss: 0.0085\n",
      "Epoch: 301, Train Loss: 0.0048, Val Loss: 0.0072\n",
      "Epoch: 302, Train Loss: 0.0046, Val Loss: 0.0071\n",
      "Epoch: 303, Train Loss: 0.0041, Val Loss: 0.0069\n",
      "Epoch: 304, Train Loss: 0.0054, Val Loss: 0.0139\n",
      "Epoch: 305, Train Loss: 0.0074, Val Loss: 0.0070\n",
      "Epoch: 306, Train Loss: 0.0054, Val Loss: 0.0067\n",
      "Epoch: 307, Train Loss: 0.0054, Val Loss: 0.0123\n",
      "Epoch: 308, Train Loss: 0.0048, Val Loss: 0.0071\n",
      "Epoch: 309, Train Loss: 0.0057, Val Loss: 0.0100\n",
      "Epoch: 310, Train Loss: 0.0049, Val Loss: 0.0063\n",
      "Epoch: 311, Train Loss: 0.0057, Val Loss: 0.0065\n",
      "Epoch: 312, Train Loss: 0.0050, Val Loss: 0.0090\n",
      "Epoch: 313, Train Loss: 0.0049, Val Loss: 0.0073\n",
      "Epoch: 314, Train Loss: 0.0072, Val Loss: 0.0082\n",
      "Epoch: 315, Train Loss: 0.0081, Val Loss: 0.0148\n",
      "Epoch: 316, Train Loss: 0.0072, Val Loss: 0.0171\n",
      "Epoch: 317, Train Loss: 0.0083, Val Loss: 0.0127\n",
      "Epoch: 318, Train Loss: 0.0087, Val Loss: 0.0163\n",
      "Epoch: 319, Train Loss: 0.0065, Val Loss: 0.0100\n",
      "Epoch: 320, Train Loss: 0.0055, Val Loss: 0.0084\n",
      "Epoch: 321, Train Loss: 0.0056, Val Loss: 0.0133\n",
      "Epoch: 322, Train Loss: 0.0055, Val Loss: 0.0070\n",
      "Epoch: 323, Train Loss: 0.0053, Val Loss: 0.0075\n",
      "Epoch: 324, Train Loss: 0.0051, Val Loss: 0.0097\n",
      "Epoch: 325, Train Loss: 0.0058, Val Loss: 0.0096\n",
      "Epoch: 326, Train Loss: 0.0049, Val Loss: 0.0071\n",
      "Epoch: 327, Train Loss: 0.0048, Val Loss: 0.0077\n",
      "Epoch: 328, Train Loss: 0.0042, Val Loss: 0.0062\n",
      "Epoch: 329, Train Loss: 0.0035, Val Loss: 0.0060\n",
      "Epoch: 330, Train Loss: 0.0034, Val Loss: 0.0065\n",
      "Epoch: 331, Train Loss: 0.0051, Val Loss: 0.0069\n",
      "Epoch: 332, Train Loss: 0.0045, Val Loss: 0.0072\n",
      "Epoch: 333, Train Loss: 0.0057, Val Loss: 0.0071\n",
      "Epoch: 334, Train Loss: 0.0051, Val Loss: 0.0068\n",
      "Epoch: 335, Train Loss: 0.0060, Val Loss: 0.0092\n",
      "Epoch: 336, Train Loss: 0.0074, Val Loss: 0.0081\n",
      "Epoch: 337, Train Loss: 0.0043, Val Loss: 0.0066\n",
      "Epoch: 338, Train Loss: 0.0057, Val Loss: 0.0097\n",
      "Epoch: 339, Train Loss: 0.0072, Val Loss: 0.0074\n",
      "Epoch: 340, Train Loss: 0.0044, Val Loss: 0.0066\n",
      "Epoch: 341, Train Loss: 0.0040, Val Loss: 0.0063\n",
      "Epoch: 342, Train Loss: 0.0038, Val Loss: 0.0080\n",
      "Epoch: 343, Train Loss: 0.0043, Val Loss: 0.0073\n",
      "Epoch: 344, Train Loss: 0.0061, Val Loss: 0.0115\n",
      "Epoch: 345, Train Loss: 0.0048, Val Loss: 0.0076\n",
      "Epoch: 346, Train Loss: 0.0057, Val Loss: 0.0070\n",
      "Epoch: 347, Train Loss: 0.0075, Val Loss: 0.0107\n",
      "Epoch: 348, Train Loss: 0.0047, Val Loss: 0.0072\n",
      "Epoch: 349, Train Loss: 0.0046, Val Loss: 0.0075\n",
      "Epoch: 350, Train Loss: 0.0083, Val Loss: 0.0106\n",
      "Epoch: 351, Train Loss: 0.0061, Val Loss: 0.0082\n",
      "Epoch: 352, Train Loss: 0.0052, Val Loss: 0.0141\n",
      "Epoch: 353, Train Loss: 0.0061, Val Loss: 0.0081\n",
      "Epoch: 354, Train Loss: 0.0076, Val Loss: 0.0127\n",
      "Epoch: 355, Train Loss: 0.0051, Val Loss: 0.0066\n",
      "Epoch: 356, Train Loss: 0.0041, Val Loss: 0.0069\n",
      "Epoch: 357, Train Loss: 0.0051, Val Loss: 0.0073\n",
      "Epoch: 358, Train Loss: 0.0061, Val Loss: 0.0081\n",
      "Epoch: 359, Train Loss: 0.0051, Val Loss: 0.0099\n",
      "Epoch: 360, Train Loss: 0.0065, Val Loss: 0.0068\n",
      "Epoch: 361, Train Loss: 0.0043, Val Loss: 0.0067\n",
      "Epoch: 362, Train Loss: 0.0047, Val Loss: 0.0067\n",
      "Epoch: 363, Train Loss: 0.0039, Val Loss: 0.0079\n",
      "Epoch: 364, Train Loss: 0.0043, Val Loss: 0.0068\n",
      "Epoch: 365, Train Loss: 0.0054, Val Loss: 0.0070\n",
      "Epoch: 366, Train Loss: 0.0041, Val Loss: 0.0067\n",
      "Epoch: 367, Train Loss: 0.0039, Val Loss: 0.0071\n",
      "Epoch: 368, Train Loss: 0.0059, Val Loss: 0.0092\n",
      "Epoch: 369, Train Loss: 0.0057, Val Loss: 0.0091\n",
      "Epoch: 370, Train Loss: 0.0057, Val Loss: 0.0097\n",
      "Epoch: 371, Train Loss: 0.0050, Val Loss: 0.0077\n",
      "Epoch: 372, Train Loss: 0.0051, Val Loss: 0.0083\n",
      "Epoch: 373, Train Loss: 0.0060, Val Loss: 0.0059\n",
      "Epoch: 374, Train Loss: 0.0042, Val Loss: 0.0066\n",
      "Epoch: 375, Train Loss: 0.0050, Val Loss: 0.0118\n",
      "Epoch: 376, Train Loss: 0.0046, Val Loss: 0.0062\n",
      "Epoch: 377, Train Loss: 0.0050, Val Loss: 0.0090\n",
      "Epoch: 378, Train Loss: 0.0069, Val Loss: 0.0108\n",
      "Epoch: 379, Train Loss: 0.0048, Val Loss: 0.0063\n",
      "Epoch: 380, Train Loss: 0.0041, Val Loss: 0.0057\n",
      "Epoch: 381, Train Loss: 0.0039, Val Loss: 0.0057\n",
      "Epoch: 382, Train Loss: 0.0039, Val Loss: 0.0069\n",
      "Epoch: 383, Train Loss: 0.0041, Val Loss: 0.0062\n",
      "Epoch: 384, Train Loss: 0.0038, Val Loss: 0.0064\n",
      "Epoch: 385, Train Loss: 0.0051, Val Loss: 0.0076\n",
      "Epoch: 386, Train Loss: 0.0040, Val Loss: 0.0067\n",
      "Epoch: 387, Train Loss: 0.0040, Val Loss: 0.0075\n",
      "Epoch: 388, Train Loss: 0.0040, Val Loss: 0.0082\n",
      "Epoch: 389, Train Loss: 0.0041, Val Loss: 0.0065\n",
      "Epoch: 390, Train Loss: 0.0034, Val Loss: 0.0073\n",
      "Epoch: 391, Train Loss: 0.0042, Val Loss: 0.0068\n",
      "Epoch: 392, Train Loss: 0.0051, Val Loss: 0.0096\n",
      "Epoch: 393, Train Loss: 0.0040, Val Loss: 0.0077\n",
      "Epoch: 394, Train Loss: 0.0041, Val Loss: 0.0070\n",
      "Epoch: 395, Train Loss: 0.0039, Val Loss: 0.0078\n",
      "Epoch: 396, Train Loss: 0.0043, Val Loss: 0.0074\n",
      "Epoch: 397, Train Loss: 0.0056, Val Loss: 0.0086\n",
      "Epoch: 398, Train Loss: 0.0055, Val Loss: 0.0078\n",
      "Epoch: 399, Train Loss: 0.0053, Val Loss: 0.0090\n",
      "Epoch: 400, Train Loss: 0.0067, Val Loss: 0.0078\n",
      "Epoch: 401, Train Loss: 0.0055, Val Loss: 0.0067\n",
      "Epoch: 402, Train Loss: 0.0054, Val Loss: 0.0070\n",
      "Epoch: 403, Train Loss: 0.0059, Val Loss: 0.0078\n",
      "Epoch: 404, Train Loss: 0.0081, Val Loss: 0.0206\n",
      "Epoch: 405, Train Loss: 0.0070, Val Loss: 0.0106\n",
      "Epoch: 406, Train Loss: 0.0068, Val Loss: 0.0155\n",
      "Epoch: 407, Train Loss: 0.0094, Val Loss: 0.0135\n",
      "Epoch: 408, Train Loss: 0.0090, Val Loss: 0.0103\n",
      "Epoch: 409, Train Loss: 0.0093, Val Loss: 0.0152\n",
      "Epoch: 410, Train Loss: 0.0072, Val Loss: 0.0080\n",
      "Epoch: 411, Train Loss: 0.0058, Val Loss: 0.0142\n",
      "Epoch: 412, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch: 413, Train Loss: 0.0060, Val Loss: 0.0081\n",
      "Epoch: 414, Train Loss: 0.0048, Val Loss: 0.0082\n",
      "Epoch: 415, Train Loss: 0.0056, Val Loss: 0.0065\n",
      "Epoch: 416, Train Loss: 0.0048, Val Loss: 0.0079\n",
      "Epoch: 417, Train Loss: 0.0054, Val Loss: 0.0141\n",
      "Epoch: 418, Train Loss: 0.0057, Val Loss: 0.0103\n",
      "Epoch: 419, Train Loss: 0.0069, Val Loss: 0.0092\n",
      "Epoch: 420, Train Loss: 0.0047, Val Loss: 0.0079\n",
      "Epoch: 421, Train Loss: 0.0043, Val Loss: 0.0080\n",
      "Epoch: 422, Train Loss: 0.0062, Val Loss: 0.0137\n",
      "Epoch: 423, Train Loss: 0.0061, Val Loss: 0.0158\n",
      "Epoch: 424, Train Loss: 0.0051, Val Loss: 0.0076\n",
      "Epoch: 425, Train Loss: 0.0056, Val Loss: 0.0084\n",
      "Epoch: 426, Train Loss: 0.0062, Val Loss: 0.0092\n",
      "Epoch: 427, Train Loss: 0.0067, Val Loss: 0.0090\n",
      "Epoch: 428, Train Loss: 0.0058, Val Loss: 0.0081\n",
      "Epoch: 429, Train Loss: 0.0040, Val Loss: 0.0089\n",
      "Epoch: 430, Train Loss: 0.0039, Val Loss: 0.0072\n",
      "Epoch: 431, Train Loss: 0.0040, Val Loss: 0.0062\n",
      "Early stopping triggered. Stopping training...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Hyperparameters for early stopping\n",
    "patience = 50  # Stop if no improvement for 30 consecutive epochs\n",
    "min_delta = 0.00001  # Minimum improvement required to reset patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Split indices for train and validation sets\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Subset datasets using the split indices\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# Data Augmentation\n",
    "for i in range(len(train_dataset)):\n",
    "    apply_random_mask(train_dataset[i], 0.4)\n",
    "    apply_random_bond_deletion(train_dataset[i], 0.4)\n",
    "\n",
    "# Create DataLoaders for the subsets\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient calculation during validation\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)  # Move data to the correct device\n",
    "            out = model(data)  # Forward pass\n",
    "\n",
    "            #out_unscaled = unstandardize(out, mean, std)\n",
    "            #target_unscaled = unstandardize(data.y, mean, std)\n",
    "\n",
    "            loss = criterion(out, data.y)  # Compute validation loss\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(val_loader.dataset)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)  # Move data to GPU or CPU\n",
    "        optimizer.zero_grad()  # Clear gradients from the last step\n",
    "        out = model(data)  # Forward pass\n",
    "        loss = criterion(out, data.y)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model weights\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Check for improvement\n",
    "    if best_val_loss - val_loss > min_delta:\n",
    "        best_val_loss = val_loss  # New best validation loss\n",
    "        patience_counter = 0  # Reset patience counter\n",
    "    else:\n",
    "        patience_counter += 1  # Increment patience counter\n",
    "\n",
    "    print(f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check if patience counter has been exceeded\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered. Stopping training...\")\n",
    "\n",
    "        break"
   ],
   "metadata": {
    "id": "70663756a41b7f64",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e32cbe11-cc74-4ff2-fe84-55213a527352",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:37:30.891591Z",
     "start_time": "2024-05-08T06:33:13.276378Z"
    }
   },
   "id": "70663756a41b7f64",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions and True Values for each node (index, prediction, true value):\n",
      "Node 0: Prediction: [11.949978   -0.02214969  0.35095352  0.26503566], True Value: [12.011     0.11      0.35      0.276144]\n",
      "Node 1: Prediction: [15.892839   -0.25835776  0.2924261   0.5563489 ], True Value: [15.9994  -0.285    0.29     0.58576]\n",
      "Node 2: Prediction: [11.95631    -0.0773719   0.35632104  0.28629813], True Value: [12.011   -0.115    0.355    0.29288]\n",
      "Node 3: Prediction: [11.939316   -0.12265817  0.3529836   0.28665897], True Value: [12.011   -0.115    0.355    0.29288]\n",
      "Node 4: Prediction: [12.000713   -0.12265129  0.35258883  0.29416057], True Value: [12.011   -0.115    0.355    0.29288]\n",
      "Node 5: Prediction: [11.918413   -0.08571138  0.36069834  0.28597465], True Value: [12.011   -0.115    0.355    0.29288]\n",
      "Node 6: Prediction: [11.949978   -0.02214969  0.35095352  0.26503566], True Value: [12.011     0.08      0.35      0.276144]\n",
      "Node 7: Prediction: [13.867122   -0.7572882   0.34348243  0.6689265 ], True Value: [14.0067  -0.78     0.33     0.71128]\n",
      "Node 8: Prediction: [11.911402    0.03806537  0.35233277  0.26146302], True Value: [12.011    -0.06      0.35      0.276144]\n",
      "Node 9: Prediction: [11.949978   -0.02214969  0.35095352  0.26503566], True Value: [12.011    -0.12      0.35      0.276144]\n",
      "Node 10: Prediction: [11.911402    0.03806537  0.35233277  0.26146302], True Value: [12.011    -0.12      0.35      0.276144]\n",
      "Node 11: Prediction: [11.917166    0.04076713  0.35295993  0.24646929], True Value: [12.011    -0.12      0.35      0.276144]\n",
      "Node 12: Prediction: [13.858301   -0.34905088  0.3424021   0.6272577 ], True Value: [14.0067  -0.46     0.325    0.71128]\n",
      "Node 13: Prediction: [11.948206    0.49863866  0.25212222  0.35290042], True Value: [12.011   0.827   0.225   0.2092]\n",
      "Node 14: Prediction: [13.909453   -0.93252957  0.35287458  0.6816689 ], True Value: [14.0067  -0.9      0.33     0.71128]\n",
      "Node 15: Prediction: [13.909453   -0.93252957  0.35287458  0.6816689 ], True Value: [14.0067  -0.9      0.33     0.71128]\n",
      "Node 16: Prediction: [11.896702    0.4840783   0.3794095   0.39282513], True Value: [12.011    0.5      0.375    0.43932]\n",
      "Node 17: Prediction: [13.92318   -0.8433888  0.3313958  0.6843537], True Value: [14.0067  -0.9      0.33     0.71128]\n",
      "Node 18: Prediction: [15.94733    -0.4893846   0.29789865  0.87760544], True Value: [15.9994  -0.5      0.296    0.87864]\n",
      "Node 19: Prediction: [11.996803   -0.13015088  0.34912956  0.28508464], True Value: [12.011   -0.115    0.355    0.29288]\n",
      "Node 20: Prediction: [35.67684    -0.03621725  0.34331948  1.2620586 ], True Value: [35.453  -0.18    0.34    1.2552]\n",
      "Node 21: Prediction: [11.854683   -0.1290301   0.35180932  0.29045722], True Value: [12.011   -0.115    0.355    0.29288]\n",
      "Node 22: Prediction: [16.011986   -0.2748692   0.292036    0.56541693], True Value: [15.9994  -0.285    0.29     0.58576]\n",
      "Node 23: Prediction: [11.949978   -0.02214969  0.35095352  0.26503566], True Value: [12.011     0.11      0.35      0.276144]\n",
      "Node 24: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.03    0.25    0.12552]\n",
      "Node 25: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.03    0.25    0.12552]\n",
      "Node 26: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.03    0.25    0.12552]\n",
      "Node 27: Prediction: [0.9855584  0.12332433 0.24386787 0.12230255], True Value: [1.008   0.115   0.242   0.12552]\n",
      "Node 28: Prediction: [0.9613497  0.13197188 0.24915928 0.12537585], True Value: [1.008   0.115   0.242   0.12552]\n",
      "Node 29: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.06    0.25    0.06276]\n",
      "Node 30: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.06    0.25    0.06276]\n",
      "Node 31: Prediction: [ 9.8257136e-01  3.8399303e-01  7.9279467e-03 -5.2233040e-04], True Value: [1.008 0.38  0.    0.   ]\n",
      "Node 32: Prediction: [0.9859139  0.06417867 0.25193894 0.10341533], True Value: [1.008   0.06    0.25    0.06276]\n",
      "Node 33: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.06    0.25    0.12552]\n",
      "Node 34: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.06    0.25    0.12552]\n",
      "Node 35: Prediction: [0.9859139  0.06417867 0.25193894 0.10341533], True Value: [1.008   0.06    0.25    0.12552]\n",
      "Node 36: Prediction: [0.9859139  0.06417867 0.25193894 0.10341533], True Value: [1.008   0.06    0.25    0.12552]\n",
      "Node 37: Prediction: [0.9918293  0.06804674 0.25507507 0.11147627], True Value: [1.008   0.06    0.25    0.12552]\n",
      "Node 38: Prediction: [0.9918293  0.06804674 0.25507507 0.11147627], True Value: [1.008   0.06    0.25    0.12552]\n",
      "Node 39: Prediction: [ 0.77793884  0.3980158  -0.00234289 -0.01005936], True Value: [1.008 0.36  0.    0.   ]\n",
      "Node 40: Prediction: [ 0.77793884  0.3980158  -0.00234289 -0.01005936], True Value: [1.008 0.36  0.    0.   ]\n",
      "Node 41: Prediction: [ 0.77793884  0.3980158  -0.00234289 -0.01005936], True Value: [1.008 0.36  0.    0.   ]\n",
      "Node 42: Prediction: [ 0.77793884  0.3980158  -0.00234289 -0.01005936], True Value: [1.008 0.36  0.    0.   ]\n",
      "Node 43: Prediction: [ 0.99775445  0.3730747  -0.00601789  0.00373483], True Value: [1.008 0.38  0.    0.   ]\n",
      "Node 44: Prediction: [ 0.99775445  0.3730747  -0.00601789  0.00373483], True Value: [1.008 0.38  0.    0.   ]\n",
      "Node 45: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.03    0.25    0.12552]\n",
      "Node 46: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.03    0.25    0.12552]\n",
      "Node 47: Prediction: [0.9834851  0.05834045 0.25008202 0.11243606], True Value: [1.008   0.03    0.25    0.12552]\n"
     ]
    }
   ],
   "source": [
    "def predict_with_indices_and_true_values(model, graph_data, device):\n",
    "    graph_data = graph_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data)\n",
    "\n",
    "    node_indices = torch.arange(graph_data.num_nodes)\n",
    "    true_values = graph_data.y\n",
    "\n",
    "    # Combine predictions with indices and true values\n",
    "    indexed_predictions = list(zip(node_indices.cpu().numpy(), predictions.cpu().numpy(), true_values.cpu().numpy()))\n",
    "    return indexed_predictions\n",
    "\n",
    "# Example usage\n",
    "graph = val_dataset[1]  # Get a single graph\n",
    "graph_predictions_and_true_values = predict_with_indices_and_true_values(model, graph, device)\n",
    "\n",
    "# Display predictions and true values for each node\n",
    "print(\"Predictions and True Values for each node (index, prediction, true value):\")\n",
    "for index, prediction, true_value in graph_predictions_and_true_values:\n",
    "    print(f\"Node {index}: Prediction: {prediction}, True Value: {true_value}\")"
   ],
   "metadata": {
    "id": "8c903a1b0957e8f7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "416e06cb-4354-43db-dbaa-246b5b12d5b4",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:40:28.782317Z",
     "start_time": "2024-05-08T06:40:28.764579Z"
    }
   },
   "id": "8c903a1b0957e8f7",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save the model and optimizer state\n",
    "torch.save(model.state_dict(), \"improved_gnn_model.pth\")"
   ],
   "metadata": {
    "id": "b56558e4c0e47cac",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:40:29.605162Z",
     "start_time": "2024-05-08T06:40:29.573566Z"
    }
   },
   "id": "b56558e4c0e47cac",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: [0.08508219 0.08847856 0.00662948 0.02404435]\n",
      "RMSE: [0.12370157 0.09422413 0.00723211 0.02615755]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def rmse_of_each_y_value(model, graph_data, device):\n",
    "    graph_data = graph_data.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data).cpu()  # Ensure predictions are moved to CPU\n",
    "\n",
    "    true_values = graph_data.y.cpu()  # Ensure true values are on CPU\n",
    "\n",
    "    # Calculate RMSE\n",
    "    mse = torch.mean((predictions - true_values) ** 2,dim = 0)  # Mean squared error\n",
    "    rmse = torch.sqrt(mse)  # Root mean squared error\n",
    "    return rmse.numpy()\n",
    "\n",
    "# Use the function across the validation dataset  \n",
    "RMSE = []\n",
    "for graph in val_dataset:\n",
    "    RMSE.append(rmse_of_each_y_value(model, graph, device))\n",
    "\n",
    "mean_rmse = np.mean(np.array(RMSE),axis = 0)\n",
    "print(\"Mean RMSE:\", mean_rmse)\n",
    "\n",
    "def sse_of_each_y_value(model, graph_data, device):\n",
    "    graph_data = graph_data.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data).cpu()  # Ensure predictions are moved to CPU\n",
    "\n",
    "    true_values = graph_data.y.cpu()  # Ensure true values are on CPU\n",
    "\n",
    "    # Calculate SSE\n",
    "    sse = torch.sum((predictions - true_values) ** 2,dim = 0)  # Sum of squared errors\n",
    "    return sse.numpy()\n",
    "\n",
    "sum_sse = 0\n",
    "for graph in val_dataset:\n",
    "    sse = sse_of_each_y_value(model, graph, device)\n",
    "    sum_sse += sse\n",
    "\n",
    "length_nodes_val = 0\n",
    "for graph in val_dataset:\n",
    "    length_nodes_val += graph.num_nodes\n",
    "\n",
    "mse = sum_sse / length_nodes_val\n",
    "RMSE = np.sqrt(mse)\n",
    "print(\"RMSE:\", RMSE)"
   ],
   "metadata": {
    "id": "31d752953c01b5ec",
    "ExecuteTime": {
     "end_time": "2024-05-08T06:40:34.492018Z",
     "start_time": "2024-05-08T06:40:30.778587Z"
    }
   },
   "id": "31d752953c01b5ec",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T06:40:34.495274Z",
     "start_time": "2024-05-08T06:40:34.493353Z"
    }
   },
   "id": "aab9700a007efab4",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a5eed65e9d2e0f23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
